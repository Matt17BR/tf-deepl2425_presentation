---
title: "Generalization Bounds"
subtitle: "Theoretical Foundations of Deep Learning"
author: "Matteo Mazzarelli"
date: 12/17/2024
date-format: long
format: 
  beamer:
    include-in-header: extra/beamer_template.tex
    theme: "Dresden"
    fonttheme: "structurebold"
    link-citations: true
    linkcolor: lmugreen
    keep-tex: true
    pdf-engine: latexmk
execute: 
  cache: true
bibliography: extra/references.bib
csl: extra/apa7-numeric-superscript-brackets-nocommas.csl
nocite: |
  @*
---

# Introduction

## Why Study Generalization?
- **Core Question**: How can models trained on limited data perform reliably on unseen scenarios?
- **Generalization** is a fundamental goal in machine learning: ensuring models extend their learned patterns to new, unseen data.
- A poorly generalized model risks:
  - **Overfitting**: Performing well on training data but poorly on unseen data.
  - **Underfitting**: Failing to capture the underlying patterns of the data.

---

## Defining Generalization
- **Supervised Learning**: Learn a function $f\!\!: X \to Y$ from labeled training data.
- **Challenge**: The learned function must perform well *beyond* the training set.
- **Evaluation**: We assess generalization by comparing model performance on training data versus a separate *testing* dataset representing unseen scenarios. This helps us understand how well the model will perform in the real world.

# Overfitting

## Demonstrating Overfitting

- **Objective**:
  - Show how increasing model complexity (polynomial degree) leads to overfitting.

- **Dataset**:
  - Using the scikit-learn **Diabetes** dataset with a single feature (BMI) and a quantitative response variable indicating disease progression (Target) [@sklearndiabetes11].

- **Approach**:
  1. Fit polynomial regression models of varying degrees.
  2. Visualize polynomial fits on the training data.
  3. Examine the fits' residuals to see how errors behave.
  4. Plot training vs. test errors to highlight overfitting.

---

```{python overfitting-plot}
#| fig-cap: "Overfitting Phenomenon in Polynomial Regression"
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_diabetes
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from matplotlib.gridspec import GridSpec

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Load the Diabetes dataset
X, y = load_diabetes(return_X_y=True)

# Use one feature for easy visualization (e.g., BMI at index 2)
X = X[:, np.newaxis, 2]

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# Polynomial degrees to test
degrees = [1, 3, 5, 7, 9]
train_errors = []
test_errors = []

for d in degrees:
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)
    
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_mse)
    test_errors.append(test_mse)

# Degrees to visualize in the top row
degrees_to_visualize = [1, 5, 9]

# Residual plots for these degrees in the second row
residual_degrees = [1, 9]

# Create figure and GridSpec
fig = plt.figure(figsize=(15, 10))
gs = GridSpec(2, 3, figure=fig)

# Top row: three polynomial fits
axes_top = [fig.add_subplot(gs[0, i]) for i in range(3)]

# Define custom colors for the top row
top_train_color = "#2a9d8f"  # teal
top_line_color = "#e76f51"   # salmon

for i, d in enumerate(degrees_to_visualize):
    poly_viz = PolynomialFeatures(degree=d)
    X_train_poly_viz = poly_viz.fit_transform(X_train)
    model_viz = LinearRegression()
    model_viz.fit(X_train_poly_viz, y_train)
    
    # Create a smooth curve for the polynomial fit
    X_curve = np.linspace(X_train.min(), X_train.max(), 200).reshape(-1, 1)
    X_curve_poly = poly_viz.transform(X_curve)
    y_curve = model_viz.predict(X_curve_poly)
    
    # Plot the training data and the polynomial fit with new colors
    axes_top[i].scatter(X_train, y_train, color=top_train_color, alpha=0.7, label='Training Data')
    axes_top[i].plot(X_curve, y_curve, color=top_line_color, linewidth=3, label=f'Degree {d}')
    axes_top[i].set_title(f'Polynomial (Degree {d})', fontsize=18, pad=10)
    axes_top[i].set_xlabel('Feature (BMI)', fontsize=16)
    axes_top[i].set_ylabel('Target', fontsize=16)
    axes_top[i].legend(fontsize=14)

# Bottom row: 2 residual plots + training vs test error
ax_res1 = fig.add_subplot(gs[1, 0])
ax_res2 = fig.add_subplot(gs[1, 1])
ax_err = fig.add_subplot(gs[1, 2])

for i, d in enumerate(residual_degrees):
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    y_test_pred = model.predict(X_test_poly)
    residuals = y_test - y_test_pred

    ax = ax_res1 if i == 0 else ax_res2
    ax.scatter(y_test_pred, residuals, color='purple', alpha=0.7)
    ax.hlines(y=0, xmin=y_test_pred.min(), xmax=y_test_pred.max(), colors='red', linestyles='dashed')
    ax.set_title(f'Residual Plot (Degree {d})', fontsize=18, pad=10)
    ax.set_xlabel('Predicted Values', fontsize=16)
    ax.set_ylabel('Residuals', fontsize=16)

# Keep the original palette for training vs test error plot
palette = sns.color_palette("deep", 3)

# Plot training vs test errors
ax_err.plot(degrees, train_errors, label='Training Error', marker='o', markersize=8, linewidth=3, color=palette[0])
ax_err.plot(degrees, test_errors, label='Test Error', marker='s', markersize=8, linewidth=3, color=palette[1])
ax_err.set_title('Training vs. Test Errors', fontsize=18, pad=10)
ax_err.set_xlabel('Polynomial Degree', fontsize=16)
ax_err.set_ylabel('Mean Squared Error', fontsize=16)
ax_err.legend(fontsize=14)

plt.tight_layout()
plt.show()
```

---

## Double Descent
- Modern machine learning introduces a fascinating twist: **Double Descent**, where increasing model complexity can lead to improved generalization after an initial overfitting phase.

![Double Descent phenomenon in a Residual Neural Network [@nakkiran19]](extra/double_descent.png){width=83%}

# Classical Bounds

## Generalization Bounds

- **Goal**: Predict a model's performance on **unseen data**.
- **Generalization Bounds** provide theoretical guarantees, linking:
  - **Generalization Error**: Error on unseen data.
  - **Empirical Risk**: Error on training data.
  - **Model Complexity**: Model's flexibility.
- **Why They Matter**: They help understand the trade-offs between:
  - **Accuracy**: How well the model fits the data.
  - **Complexity**: Ability to model intricate patterns.
  - **Data Size**: Amount of data needed for reliable learning.

---

## Hoeffding's Inequality

- **What it is**: A probabilistic tool that helps estimate how well a model will generalize.
- **Focus**: Quantifies the difference between **empirical risk** (training error) and **generalization error** (true error) for a *single, fixed model*.

---

## Hoeffding's Inequality: The Math

- **Expression**[@mohri12]:
  $$
  P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2n\varepsilon^2)
  $$
- **Breakdown**:
  - $R(h)$: The **true risk** of hypothesis $h$, defined as the expected loss over the data distribution: $R(h) = \mathbb{E}_{x,y \sim D}[\ell(h(x), y)]$.
  - $R_{\text{emp}}(h)$: The **empirical risk** of hypothesis $h$, defined as the average loss over the training dataset $S$ of size $n$: $R_{\text{emp}}(h) = \frac{1}{n} \sum_{i=1}^{n} \ell(h(x_i), y_i)$.
  - $\varepsilon$: Error tolerance.
  - $n$: Dataset size.

---

## Hoeffding's Inequality: Convergence

- **Rate of Convergence**: Simulating biased coin flips to show the rate at which sample mean approaches the true probability.
- **Hoeffding's Bound**, derived from the $\exp(-2n\varepsilon^2)$ term, shows **faster convergence** as $n$ increases.

```{python convergence-plot}  
#| fig-cap: "Convergence to True Probability with Hoeffding Bounds"
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# True probability of heads (unknown in practice)
true_p_heads = 0.6

# Number of coin flips (dataset sizes)
n_flips = np.arange(1, 401)

# Number of simulations
n_simulations = 10

# Store results
sample_means = np.zeros((n_simulations, len(n_flips)))

# Run simulations
for i in range(n_simulations):
    for j, n in enumerate(n_flips):
        # Simulate coin flips (1 for heads, 0 for tails)
        flips = np.random.binomial(1, true_p_heads, size=n)
        # Calculate sample mean (estimate of p_heads)
        sample_means[i, j] = np.mean(flips)

# Calculate Hoeffding bounds (epsilon) for a fixed confidence (e.g., 95%)
confidence = 0.95
delta = 1 - confidence
epsilon = np.sqrt(np.log(2 / delta) / (2 * n_flips))

# Create the plot
plt.figure(figsize=(11, 2.75))

# Plot each simulation
for i in range(n_simulations):
    plt.plot(n_flips, sample_means[i, :], color='skyblue', alpha=0.3)

# Plot the true probability
plt.axhline(y=true_p_heads, color='red', linestyle='--', label='True Probability')

# Plot Hoeffding bounds with clipping
lower_bound = np.clip(true_p_heads - epsilon, 0, 1)  # Clip lower bound at 0
upper_bound = np.clip(true_p_heads + epsilon, 0, 1)  # Clip upper bound at 1
plt.fill_between(n_flips, lower_bound, upper_bound, color='lightcoral', alpha=0.3, label='Hoeffding Bound (95% Confidence)')

plt.xlabel('Number of Coin Flips (n)')
plt.ylabel('P(Heads) Estimate')
# plt.title('Convergence of Sample Mean to True Probability')
plt.xlim(0, 400)
plt.ylim(0, 1)  # Set y-axis limits to 0 and 1
plt.legend()
plt.show()
```

---

## Hoeffding's Inequality: Interpretation

- The probability of a large difference between the true risk (generalization error) and the empirical risk (training error) decreases **exponentially** with:
  - **Larger datasets** ($n$).
  - **Smaller error tolerance** ($\varepsilon$).
- **Note**: Hoeffding's inequality applies more generally to the difference between the sample average and the expectation of any bounded random variable. We have shown a special application of the inequality.
- **Limitations**: We usually pick the best model from many, not just one. Hoeffding doesn't account for how complex the model class is.

---

## Union Bound

- **What it does**: Extends bounds like Hoeffding's to work when choosing from **many models** (a hypothesis space $\mathcal{H}$).
- **Main Idea**: Considers the chance that *at least one* model in $\mathcal{H}$ has a large difference between training and true error.

---

## Union Bound: The Math

- **Expression**[@samir16]:
  $$
  P\left(\sup_{h \in \mathcal{H}} |R(h) - R_{\text{emp}}(h)| > \epsilon \right) \leq \sum_{h \in \mathcal{H}} P\left(|R(h) - R_{\text{emp}}(h)| > \epsilon \right)
  $$
- **Breakdown**:
  - $R(h)$: True risk (expected loss).
  - $R_{\text{emp}}(h)$: Empirical risk (average training loss).
  - $\sup_{h \in \mathcal{H}}$: Account for the worst-case scenario across all hypotheses, considering the largest deviation between true and empirical risk.
  - $\sum_{h \in \mathcal{H}}$: Sums up probabilities of large error differences for each model in the hypothesis space $\mathcal{H}$.

---

## Union Bound: Interpretation

- **Larger Model Space**: The more models we consider, the looser the bound becomes.

| **Hypothesis Space Size** | **Bound**      | **Model Capacity** |
| :--------------------- | :--------- | :------------- |
| Small                 | Tighter    | Limited        |
| Large                 | Looser     | Higher         |
: Trade-off: Hypothesis Space vs. Bound & Capacity

---

## Moving Forward

- **Challenge**: Real-world model spaces are often infinite or too large.
- **Solution**: We need ways to measure model complexity that go beyond counting.
- **Next**: Exploring **complexity measures** for more practical generalization bounds.

# Advanced Bounds

## Why Advanced Bounds?

- **Classical Bounds** give us a good starting point, but they can be loose.
- **Goal**: Tighter bounds that better reflect real-world performance.
- **How?**: By measuring model complexity in more sophisticated ways.

---

## VC Dimension

- **Growth Function** ($\Pi_{\mathcal{H}}(n)$): How many ways can a model class ($\mathcal{H}$) label *n* data points?
  - More ways = more complex.
  - For small *n*, $\Pi_{\mathcal{H}}(n)=2^n$. 
- **Shattering**: A model class *shatters* a dataset if it can label it in *every possible way*.

---

## VC Dimension: Definition

- **VC Dimension ($d_{\text{VC}}$)**: The size of the *largest* dataset a model class can shatter.
- **Example**: Linear classifiers in 2D have $d_{\text{VC}} = 3$. They can shatter 3 points but not 4 (in all configurations).

```{python vc-dimension-illustration}
#| fig-cap: "VC Dimension of Linear Classifiers in 2D"
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Create a figure and axes for subplots
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Function to plot points and a linear classifier
def plot_points(ax, points, labels, title):
    ax.scatter(points[:, 0], points[:, 1], c=labels, cmap='viridis', s=100, edgecolors='k')
    ax.set_title(title)
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')

    # Add a linear classifier (for illustration)
    if len(np.unique(labels)) > 1:
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()
        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),
                             np.linspace(ylim[0], ylim[1], 100))
        # Assume a simple linear classifier (y = x) for demonstration
        Z = xx - yy
        ax.contour(xx, yy, Z, levels=[0], colors='red')

# --- 3 Points, Shattered ---
points_3 = np.array([[1, 2], [2, 1], [3, 3]])
labels_3 = np.array([1, -1, 1])  # Example labels that can be separated
plot_points(axes[0], points_3, labels_3, "3 Points, Shattered")

# --- 4 Points, Not Shattered (XOR Configuration) ---
points_4 = np.array([[1, 1], [1, 2], [2, 1], [2, 2]])
labels_4 = np.array([1, -1, -1, 1])  # XOR configuration, cannot be linearly separated
plot_points(axes[1], points_4, labels_4, "4 Points, Not Shattered (XOR)")

# --- 4 Points, Shattered (Convex Configuration) ---
points_4_2 = np.array([[0, 0], [0, 2], [2, 2], [1.5, 0.5]])
labels_4_2 = np.array([1, 1, -1, -1])  # Example labels that can be separated
plot_points(axes[2], points_4_2, labels_4_2 , "4 Points, Shattered (Convex)")

plt.tight_layout()
plt.show()
```

- **Intuition**: Higher $d_{\text{VC}}$ = more complex model.

---

## VC Generalization Bound: The Math

- **Expression**[@vapnik95]:
  $$
  R(h) \leq R_{\text{emp}}(h) + \sqrt{\frac{8 d_{\text{VC}} \left(\ln\left(\frac{2n}{d_{\text{VC}}}\right) + 1\right) + 8 \ln\left(\frac{4}{\delta}\right)}{n}}
  $$
- **Breakdown**:
  - $R(h)$: True risk (expected loss).
  - $R_{\text{emp}}(h)$: Empirical risk (average training loss).
  - $d_{\text{VC}}$: VC dimension.
  - $n$: Dataset size.
  - $\delta$: Confidence parameter.

---

## VC Generalization Bound: Interpretation

- **Higher VC Dimension**:
  - More complex model, looser bound, higher risk of overfitting.
- **Larger Dataset**:
  - Tighter bound, better generalization.

```{python vc-bound-insights}
#| fig-cap: "Approximation of the VC Generalization Bound"
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Function to calculate the VC bound term
def vc_bound_term(d_vc, n, delta=0.05):
    return np.sqrt((8 * d_vc * (np.log((2 * n) / d_vc) + 1) + 8 * np.log(4 / delta)) / n)

# Dataset sizes
m_values = np.linspace(10, 1000, 100)

# VC dimension values
d_vc_values = [1, 5, 10]

# Create the plot
plt.figure(figsize=(10, 2.25))

# Plotting for different VC dimension values
for d_vc in d_vc_values:
    bound_values = vc_bound_term(d_vc, m_values)
    plt.plot(m_values, bound_values, label=f'VC Dimension = {d_vc}')

plt.xlabel('Dataset Size (n)')
plt.ylabel('VC Bound Term')
# plt.title('Impact of VC Dimension and Dataset Size on Generalization Bound')
plt.legend()
plt.show()
```

---

## Distribution-Based Bounds

- **VC theory** often considers the *worst-case* scenario.
- **New Idea**: Use information about the **data distribution** for tighter bounds.
- **Example**: Support Vector Machines (SVMs).
  - **Margin**: Distance from the decision boundary to the nearest data points.
  - Larger margin = better generalization.
- **Benefit**: More realistic bounds reflecting real-world performance.

---

## More Measures of Complexity

- **Why?**: VC dimension can be too pessimistic.
- **Goal**: More nuanced measures, especially for things like neural networks.

| **Measure**                | **Description**                                  | **Key Idea**                                    |
| :---------------------- | :--------------------------------------------- | :------------------------------------------ |
| Covering Numbers   | How many "balls" cover the hypothesis space?      | Smaller = simpler = tighter bounds         |
| \vspace{1pt} Rademacher Complexity | \vspace{1pt} How well can the model fit random noise?          | \vspace{1pt} Lower = less prone to overfitting          |
: Further ways to measure complexity [@bousquet03]

# Conclusions

## Key Takeaways I

- **Generalization** is crucial: We want models to work on **unseen data**, not just the training set.
- **Overfitting** is a risk: More complex models can memorize the training data but fail to generalize.
- **Classical Bounds** highlight the importance of:
  - **Dataset size**: More data leads to better generalization.
  - **Model complexity**: Simpler models (smaller hypothesis spaces) are safer.

---

## Key Takeaways II

- **Advanced Bounds** offer a refined view:
  - **VC Dimension**: Measures a model's ability to shatter data. Higher VC dimension means more complexity.
  - **Distribution-Based**: Leverage data properties for tighter bounds.
- **The Goal**: Balance model expressiveness with the risk of overfitting by controlling complexity and leveraging insights from the data distribution.

---

### References

::: {#refs}
\footnotesize
:::