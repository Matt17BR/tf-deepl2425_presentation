---
title: "Generalization Bounds"
subtitle: "Theoretical Foundations of Deep Learning"
author: "Matteo Mazzarelli"
date: 12/17/2024
date-format: long
format: 
  beamer:
    theme: "Dresden"
    fonttheme: "structurebold"
    link-citations: true
    keep-tex: true
    include-in-header: extra/beamer_template.tex
    pdf-engine: latexmk
bibliography: extra/references.bib
csl: extra/apa-numeric-superscript-brackets.csl
nocite: |
  @*
---

# Introduction

## Motivation
- **Core Question**: How can models trained on limited data perform reliably on unseen scenarios?
- **Generalization** is a fundamental goal in machine learning: ensuring models extend their learned patterns to new, unseen data.
- A poorly generalized model risks:
  - **Overfitting**: Performing well on training data but poorly on unseen data.
  - **Underfitting**: Failing to capture the underlying patterns of the data.

---

## The Learning Problem
- **Supervised Learning**: 
  - Goal: Learn a function $f: X \to Y$ mapping inputs $X$ to outputs $Y$ based on labeled training data.
- **Key Question**: Can the learned function perform well on unseen data?
- **Generalization**:
  - Ability of a model to extend its learning beyond the training data.
  - **Central Problem** in machine learning: balancing *empirical performance* with *future predictions*.

# Overfitting

## Demonstrating Overfitting

- **Objective**:
  - Show how increasing model complexity (polynomial degree) leads to overfitting.

- **Dataset**:
  - Using the **Diabetes** dataset with a single feature (BMI).

- **Approach**:
  - Fit polynomial regression models of varying degrees.
  - Visualize polynomial fits on the training data.
  - Examine the fits' residuals to see how errors behave.
  - Plot training vs. test errors to highlight overfitting.

---

```{python overfitting-plot}
#| fig-cap: "Overfitting Phenomenon in Polynomial Regression"
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_diabetes
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from matplotlib.gridspec import GridSpec

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Load the Diabetes dataset
X, y = load_diabetes(return_X_y=True)

# Use one feature for easy visualization (e.g., BMI at index 2)
X = X[:, np.newaxis, 2]

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Polynomial degrees to test
degrees = [1, 3, 5, 7, 9]
train_errors = []
test_errors = []

for d in degrees:
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)
    
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_mse)
    test_errors.append(test_mse)

# Degrees to visualize in the top row
degrees_to_visualize = [1, 5, 9]

# Residual plots for these degrees in the second row
residual_degrees = [1, 9]

# Create figure and GridSpec
fig = plt.figure(figsize=(15, 10))
gs = GridSpec(2, 3, figure=fig)

# Top row: three polynomial fits
axes_top = [fig.add_subplot(gs[0, i]) for i in range(3)]

# Define custom colors for the top row
top_train_color = "#2a9d8f"  # teal
top_line_color = "#e76f51"   # salmon

for i, d in enumerate(degrees_to_visualize):
    poly_viz = PolynomialFeatures(degree=d)
    X_train_poly_viz = poly_viz.fit_transform(X_train)
    model_viz = LinearRegression()
    model_viz.fit(X_train_poly_viz, y_train)
    
    # Create a smooth curve for the polynomial fit
    X_curve = np.linspace(X_train.min(), X_train.max(), 200).reshape(-1, 1)
    X_curve_poly = poly_viz.transform(X_curve)
    y_curve = model_viz.predict(X_curve_poly)
    
    # Plot the training data and the polynomial fit with new colors
    axes_top[i].scatter(X_train, y_train, color=top_train_color, alpha=0.7, label='Training Data')
    axes_top[i].plot(X_curve, y_curve, color=top_line_color, linewidth=3, label=f'Degree {d}')
    axes_top[i].set_title(f'Polynomial Degree {d}', fontsize=18, pad=10)
    axes_top[i].set_xlabel('Feature (BMI)', fontsize=14)
    axes_top[i].set_ylabel('Target', fontsize=14)
    axes_top[i].legend(fontsize=12)

# Bottom row: 2 residual plots + training vs test error
ax_res1 = fig.add_subplot(gs[1, 0])
ax_res2 = fig.add_subplot(gs[1, 1])
ax_err = fig.add_subplot(gs[1, 2])

for i, d in enumerate(residual_degrees):
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    y_test_pred = model.predict(X_test_poly)
    residuals = y_test - y_test_pred

    ax = ax_res1 if i == 0 else ax_res2
    ax.scatter(y_test_pred, residuals, color='purple', alpha=0.7)
    ax.hlines(y=0, xmin=y_test_pred.min(), xmax=y_test_pred.max(), colors='red', linestyles='dashed')
    ax.set_title(f'Residual Plot (Degree={d})', fontsize=16, pad=10)
    ax.set_xlabel('Predicted Values', fontsize=14)
    ax.set_ylabel('Residuals', fontsize=14)

# Keep the original palette for training vs test error plot
palette = sns.color_palette("deep", 3)

# Plot training vs test errors
ax_err.plot(degrees, train_errors, label='Training Error', marker='o', markersize=8, linewidth=3, color=palette[0])
ax_err.plot(degrees, test_errors, label='Test Error', marker='s', markersize=8, linewidth=3, color=palette[1])
ax_err.set_title('Training vs Test Errors', fontsize=18, pad=10)
ax_err.set_xlabel('Polynomial Degree', fontsize=14)
ax_err.set_ylabel('Mean Squared Error', fontsize=14)
ax_err.legend(fontsize=12)

plt.tight_layout()
plt.show()
```

---

## Double Descent
- Modern machine learning introduces a fascinating twist: **Double Descent**, where increasing model complexity can lead to improved generalization after an initial overfitting phase.

![Double Descent phenomenon in a Residual Neural Network [@nakkiran19]](extra/double_descent.png){width=80%}

# Classical Bounds

## Introducing Generalization Bounds
- **What Are Generalization Bounds?**
  - Theoretical tools offering guarantees about a model's performance on unseen data.
  - Relate:
    - **Generalization Error**: How well the model performs on unseen data.
    - **Empirical Risk**: Performance observed on training data.
    - **Model Complexity**: How expressive the model is.

---

- **Purpose**:
  - Provide insights into the trade-offs between:
    - **Model Accuracy**: How well the model captures the data patterns.
    - **Model Complexity**: The expressiveness of the model and its ability to fit intricate patterns.
    - **Training Data Size**: How much data is required to achieve reliable generalization.

---

## Hoeffding's Inequality: A Starting Point
- **What is Hoeffding's Inequality?**
  - A fundamental result in probability theory used to bound the difference between the **empirical risk** and the **generalization error** for a fixed hypothesis.
  - Provides a way to measure how closely a model's performance on training data reflects its performance on unseen data.

---

## Mathematical Formulation of Hoeffding's Inequality
- **Hoeffding's Inequality**:
  $$
  P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
  $$
  - $R(h)$: Generalization error (true performance on unseen data).
  - $R_{\text{emp}}(h)$: Empirical risk (error on training data).
  - $\varepsilon$: A small positive value (tolerance).
  - $m$: Size of the dataset.

---

### Key Insights
- The probability that the generalization error $R(h)$ deviates significantly from the empirical risk $R_{\text{emp}}(h)$ decreases **exponentially** with:
  - Larger dataset size $m$.
  - Smaller tolerance $\varepsilon$.

---

## Rates of Convergence
- **What Are Rates of Convergence?**
  - Quantify how quickly the generalization error approaches the empirical risk as the dataset size $m$ grows.
  - Provide a guideline for determining the dataset size needed to achieve a desired level of generalization.
  - In Hoeffding's inequality:
    $$
    P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
    $$
    - The **exponential term** $\exp(-2m\varepsilon^2)$ shows that the convergence is faster with larger datasets.

- **Key Factors**:
  - **Dataset Size ($m$)**: Larger datasets reduce the gap between $R(h)$ and $R_{\text{emp}}(h)$ more quickly.
  - **Tolerance ($\varepsilon$)**: Smaller tolerances require larger datasets for the same level of confidence.
  
---

## Interpretation of Hoeffding's Inequality
- **What Does It Mean?**
  - As the dataset size ($m$) increases, the empirical risk becomes a more reliable indicator of the generalization error.
  - For a fixed hypothesis, we can be confident that the performance observed on training data is close to what can be expected on unseen data.
  - The **rate of convergence** shows how quickly this reliability improves as $m$ grows.

---

### Key Insights
- Hoeffding's inequality gives a **quantitative guarantee** about the relationship between training performance and unseen data performance.
- Understanding convergence rates helps in planning how much data is needed for robust generalization.

---

## Limitations of Hoeffding's Inequality
- **Beyond Fixed Hypotheses**:
  - Hoeffding's inequality assumes a single, fixed hypothesis.
  - In practice, machine learning involves selecting the best hypothesis from a **large hypothesis class** $\mathcal{H}$, increasing the risk of overfitting.

- **Need for Complexity-Aware Bounds**:
  - Simple bounds like Hoeffding’s do not consider the complexity of the hypothesis class, which influences generalization.

---

## The Union Bound
- **What is the Union Bound?**
  - A probability tool used to extend bounds like Hoeffding's inequality to apply across an entire hypothesis space $\mathcal{H}$.
  - Helps estimate the probability that **at least one hypothesis** in $\mathcal{H}$ has a large generalization gap.
- **Key Idea**:
  - Instead of considering a single fixed hypothesis, the Union Bound aggregates the probabilities of generalization gaps over all hypotheses in $\mathcal{H}$.

---

## Formalization of The Union Bound
- **Mathematical Expression**:
  $$
  P\left(\sup_{h \in \mathcal{H}} |R(h) - R_{\text{emp}}(h)| > \epsilon \right) \leq \sum_{h \in \mathcal{H}} P\left(|R(h) - R_{\text{emp}}(h)| > \epsilon \right)
  $$
  - $\sup_{h \in \mathcal{H}}$: The supremum ensures we account for the worst-case scenario across all hypotheses.
  - $P(|R(h) - R_{\text{emp}}(h)| > \epsilon)$: The probability of a significant generalization gap for each hypothesis.

- **How It Works**:
  - By summing up the probabilities for all hypotheses, the Union Bound provides a way to analyze the worst-case scenario over the hypothesis space.

---

## Implications of The Union Bound
- **Impact of Hypothesis Space Size**:
  - The bound depends directly on the **size of the hypothesis space** $|\mathcal{H}|$.
  - Larger hypothesis spaces increase the sum, making the bound looser.

- **Takeaway**:
  - The Union Bound highlights a trade-off:
    - **Small hypothesis space**: Tighter bounds, but limited model capacity.
    - **Large hypothesis space**: Higher capacity, but risk of overfitting and looser bounds.

---

## Transition to Advanced Bounds
- **Connection to Practical Learning**:
  - In practice, hypothesis spaces are often infinite or too large to enumerate explicitly. This motivates the need for alternative ways to measure hypothesis complexity.
- **From Simple to Sophisticated**:
  - The Union Bound provides a conceptual basis for understanding how hypothesis space size affects generalization.
  - Next, we delve into **complexity measures** that allow us to extend generalization bounds to more practical, infinite hypothesis spaces.

# Advanced Bounds

## Motivation for Advanced Bounds
- **Advanced bounds** address a variety of the limitations we have outlined by incorporating:
  - **VC Dimension**: A measure of the capacity or expressiveness of a hypothesis class. Higher VC dimensions indicate more complex models, which may require more data to generalize well.
  - **Rademacher Complexity**: A data-dependent measure of how well a hypothesis class can fit random noise in the training data. It captures both the hypothesis class and the specifics of the data distribution.

---

- **Extending Convergence Rates**:
  - Advanced bounds refine the rates of convergence by linking the generalization error to:
    - The size of the dataset $m$.
    - The complexity of the hypothesis class (e.g., **VC dimension** or **Rademacher complexity**).
  - For example, the generalization error is often bounded as:
    $$
    R(h) - R_{\text{emp}}(h) \leq \mathcal{O}\left(\sqrt{\frac{\text{Complexity}(\mathcal{H})}{m}}\right)
    $$
    - Larger datasets $m$ reduce error, but higher complexity increases the required data for a desired level of generalization.

- **Practical Implications**:
  - These bounds provide actionable insights for balancing model complexity and dataset size.

## Vapnik-Chervonenkis (VC) Theory
- **Growth Function**
  - The **Growth Function** is a measure of the expressiveness of a hypothesis space $\mathcal{H}$.
  - **Definition**:
    - The growth function, $\Pi_{\mathcal{H}}(m)$, is the maximum number of distinct ways a hypothesis space can label $m$ data points.
  - **Key Idea**:
    - A more expressive hypothesis space can label datasets in a greater number of ways, indicating higher complexity.
  - **Growth Behavior**:
    - For small $m$, $\Pi_{\mathcal{H}}(m) = 2^m$.
    - For larger $m$, the growth may be limited by the structure of $\mathcal{H}$.

---

- **VC Dimension**
  - The **VC Dimension** is a scalar value that quantifies the capacity of a hypothesis space $\mathcal{H}$.
  - **Definition**:
    - The VC dimension $d_{\text{VC}}$ is the size of the largest dataset that can be **shattered** by $\mathcal{H}$.
  - **Shattering**:
    - A dataset is shattered if every possible labeling of the dataset can be perfectly captured by hypotheses in $\mathcal{H}$.
- **Examples**:
  - A linear classifier in 2D space has a VC dimension of 3 (it can shatter any 3 points, but not all configurations of 4 points).

---

## VC Generalization Bound
- **What is the VC Generalization Bound?**
  - A theoretical result that connects the **generalization error** with the **empirical risk**, the **VC dimension**, and the size of the dataset.
  - **Mathematical Formulation**:
    $$
    R(h) \leq R_{\text{emp}}(h) + \sqrt{\frac{8 d_{\text{VC}} \left(\ln\left(\frac{2m}{d_{\text{VC}}}\right) + 1\right) + 8 \ln\left(\frac{4}{\delta}\right)}{m}}
    $$
    - $R(h)$: Generalization error.
    - $R_{\text{emp}}(h)$: Empirical risk.
    - $d_{\text{VC}}$: VC dimension.
    - $m$: Dataset size.
    - $\delta$: Confidence level ($1 - \delta$ is the probability that the bound holds).

---

### Key Insights
- As $d_{\text{VC}}$ increases (more complex hypothesis space):
  - The bound becomes looser, reflecting a higher risk of overfitting.
- As $m$ increases (larger dataset size):
  - The bound tightens, improving generalization guarantees.

---

## Summing Up VC Theory
- **Expressiveness vs. Generalization**:
  - The VC dimension captures the **expressiveness** of a hypothesis space:
    - Higher $d_{\text{VC}}$: More complex, more expressive.
  - A balance is required to avoid overfitting (high complexity) or underfitting (low complexity).
- **Implications for Learning**:
  - The VC dimension helps understand:
    - Why simpler models often generalize better.
    - Why increasing data size improves generalization, especially for complex models.
- **Foundation for Algorithm Design**:
  - VC theory guides the development of learning algorithms by quantifying the trade-offs between hypothesis complexity, data size, and generalization performance.

## Distribution-Based Bounds
- **From General Bounds to Data-Driven Insights**:
  - Generalization bounds like Hoeffding’s inequality and VC bounds rely on worst-case scenarios.
- **Distribution-Based Bounds**:
  - Leverage specific properties of the data distribution to achieve **tighter bounds**.
  - Exploit **data structure** to understand how well a model generalizes in practice.

---

## Example: Support Vector Machines (SVMs)
- **SVMs and Margin-Based Bounds**:
  - Support Vector Machines (SVMs) introduce the concept of a **margin**, the distance between the decision boundary and the nearest data points.
  - **Intuition**:
    - A larger margin indicates better separation between classes, leading to better generalization.
  - **Margin-Based Generalization Bounds**:
    - Generalization error decreases as the margin increases, even for infinite hypothesis spaces.

---

## Alternative Capacity Measures
- **Why Explore Alternative Measures?**
  - VC dimension assumes worst-case datasets, often leading to overly conservative bounds.
  - Alternative measures provide more nuanced insights into hypothesis space complexity, especially for modern machine learning models like neural networks.

---

- **Examples of Alternative Measures**
  1. **Covering Numbers**: The minimum number of small "balls" needed to cover the hypothesis space under a certain metric. Smaller covering numbers indicate a simpler hypothesis space, leading to tighter generalization bounds.
  2. **Rademacher Complexity**: Measures the ability of a hypothesis class to fit random noise. A lower Rademacher complexity indicates that the hypothesis space is less prone to overfitting.

- **Next Steps**:
  - We want to explore how these theoretical tools are applied to modern machine learning methods.

---

### Key Insights
- These measures allow for tighter, data-adaptive generalization bounds, particularly useful for complex or large-scale models.
- There’s no one-size-fits-all measure. The choice of capacity measure depends on:
  - The hypothesis space.
  - The structure of the data.
  - The learning algorithm.

# Conclusion

---

### References

::: {#refs}
:::