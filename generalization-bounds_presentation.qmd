---
title: "Generalization Bounds"
subtitle: "Theoretical Foundations of Deep Learning"
author: "Matteo Mazzarelli"
date: 12/17/2024
date-format: long
format: 
  beamer:
    include-in-header: extra/beamer_template.tex
    theme: "Dresden"
    fonttheme: "structurebold"
    link-citations: true
    linkcolor: lmugreen
    keep-tex: true
    pdf-engine: latexmk
execute: 
  cache: true
bibliography: extra/references.bib
csl: extra/apa7-numeric-superscript-brackets-nocommas.csl
nocite: |
  @*
---

# Introduction

## Why Study Generalization?
- **Core Question**: How can models trained on limited data perform reliably on unseen scenarios?
- **Generalization** is a fundamental goal in machine learning: ensuring models extend their learned patterns to new, unseen data.
- A poorly generalized model risks:
  - **Overfitting**: Performing well on training data but poorly on unseen data.
  - **Underfitting**: Failing to capture the underlying patterns of the data.

---

## Defining Generalization
- **Supervised Learning**: 
  - Goal: Learn a function $f: X \to Y$ mapping inputs $X$ to outputs $Y$ based on labeled training data.
- **Key Question**: Can the learned function perform well on unseen data?
- **Generalization**:
  - Ability of a model to extend its learning beyond the training data.
  - **Central problem** in machine learning: balancing *empirical performance* with *future predictions*.

# Overfitting

## Demonstrating Overfitting

- **Objective**:
  - Show how increasing model complexity (polynomial degree) leads to overfitting.

- **Dataset**:
  - Using the scikit-learn **Diabetes** dataset with a single feature (BMI) and a quantitative response variable indicating disease progression (Target) [@sklearndiabetes11].

- **Approach**:
  1. Fit polynomial regression models of varying degrees.
  2. Visualize polynomial fits on the training data.
  3. Examine the fits' residuals to see how errors behave.
  4. Plot training vs. test errors to highlight overfitting.

---

```{python overfitting-plot}
#| fig-cap: "Overfitting Phenomenon in Polynomial Regression"
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import load_diabetes
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from matplotlib.gridspec import GridSpec

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Load the Diabetes dataset
X, y = load_diabetes(return_X_y=True)

# Use one feature for easy visualization (e.g., BMI at index 2)
X = X[:, np.newaxis, 2]

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)

# Polynomial degrees to test
degrees = [1, 3, 5, 7, 9]
train_errors = []
test_errors = []

for d in degrees:
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)
    
    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)
    
    train_mse = mean_squared_error(y_train, y_train_pred)
    test_mse = mean_squared_error(y_test, y_test_pred)
    
    train_errors.append(train_mse)
    test_errors.append(test_mse)

# Degrees to visualize in the top row
degrees_to_visualize = [1, 5, 9]

# Residual plots for these degrees in the second row
residual_degrees = [1, 9]

# Create figure and GridSpec
fig = plt.figure(figsize=(15, 10))
gs = GridSpec(2, 3, figure=fig)

# Top row: three polynomial fits
axes_top = [fig.add_subplot(gs[0, i]) for i in range(3)]

# Define custom colors for the top row
top_train_color = "#2a9d8f"  # teal
top_line_color = "#e76f51"   # salmon

for i, d in enumerate(degrees_to_visualize):
    poly_viz = PolynomialFeatures(degree=d)
    X_train_poly_viz = poly_viz.fit_transform(X_train)
    model_viz = LinearRegression()
    model_viz.fit(X_train_poly_viz, y_train)
    
    # Create a smooth curve for the polynomial fit
    X_curve = np.linspace(X_train.min(), X_train.max(), 200).reshape(-1, 1)
    X_curve_poly = poly_viz.transform(X_curve)
    y_curve = model_viz.predict(X_curve_poly)
    
    # Plot the training data and the polynomial fit with new colors
    axes_top[i].scatter(X_train, y_train, color=top_train_color, alpha=0.7, label='Training Data')
    axes_top[i].plot(X_curve, y_curve, color=top_line_color, linewidth=3, label=f'Degree {d}')
    axes_top[i].set_title(f'Polynomial (Degree {d})', fontsize=18, pad=10)
    axes_top[i].set_xlabel('Feature (BMI)', fontsize=16)
    axes_top[i].set_ylabel('Target', fontsize=16)
    axes_top[i].legend(fontsize=14)

# Bottom row: 2 residual plots + training vs test error
ax_res1 = fig.add_subplot(gs[1, 0])
ax_res2 = fig.add_subplot(gs[1, 1])
ax_err = fig.add_subplot(gs[1, 2])

for i, d in enumerate(residual_degrees):
    poly = PolynomialFeatures(degree=d)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    model = LinearRegression()
    model.fit(X_train_poly, y_train)
    
    y_test_pred = model.predict(X_test_poly)
    residuals = y_test - y_test_pred

    ax = ax_res1 if i == 0 else ax_res2
    ax.scatter(y_test_pred, residuals, color='purple', alpha=0.7)
    ax.hlines(y=0, xmin=y_test_pred.min(), xmax=y_test_pred.max(), colors='red', linestyles='dashed')
    ax.set_title(f'Residual Plot (Degree {d})', fontsize=18, pad=10)
    ax.set_xlabel('Predicted Values', fontsize=16)
    ax.set_ylabel('Residuals', fontsize=16)

# Keep the original palette for training vs test error plot
palette = sns.color_palette("deep", 3)

# Plot training vs test errors
ax_err.plot(degrees, train_errors, label='Training Error', marker='o', markersize=8, linewidth=3, color=palette[0])
ax_err.plot(degrees, test_errors, label='Test Error', marker='s', markersize=8, linewidth=3, color=palette[1])
ax_err.set_title('Training vs. Test Errors', fontsize=18, pad=10)
ax_err.set_xlabel('Polynomial Degree', fontsize=16)
ax_err.set_ylabel('Mean Squared Error', fontsize=16)
ax_err.legend(fontsize=14)

plt.tight_layout()
plt.show()
```

---

## Double Descent
- Modern machine learning introduces a fascinating twist: **Double Descent**, where increasing model complexity can lead to improved generalization after an initial overfitting phase.

![Double Descent phenomenon in a Residual Neural Network [@nakkiran19]](extra/double_descent.png){width=80%}

# Classical Bounds

## Generalization Bounds

- **Goal**: Predict a model's performance on **unseen data**.
- **Generalization Bounds** provide theoretical guarantees, linking:
  - **Generalization Error**: Error on unseen data.
  - **Empirical Risk**: Error on training data.
  - **Model Complexity**: Model's flexibility.
- **Why They Matter**: They help understand the trade-offs between:
  - **Accuracy**: How well the model fits the data.
  - **Complexity**: Ability to model intricate patterns.
  - **Data Size**: Amount of data needed for reliable learning.

---

## Hoeffding's Inequality

- **What it is**: A probabilistic tool that helps estimate how well a model will generalize.
- **Focus**: Quantifies the difference between **empirical risk** (training error) and **generalization error** (true error) for a *single, fixed model*.

---

## Hoeffding's Inequality: The Math

- **Formula**[@mohri12]:
  $$
  P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
  $$

  - $R(h)$: True error on unseen data.
  - $R_{\text{emp}}(h)$: Error on training data.
  - $\varepsilon$: Error tolerance.
  - $m$: Dataset size.

- **Interpretation**: The probability of a large difference between true error and training error decreases **exponentially** with:
  - **Larger datasets** ($m$).
  - **Smaller error tolerance** ($\varepsilon$).

---

## Hoeffding's Inequality: Convergence

- **Rate of Convergence**: How quickly the training error becomes a good estimate of the true error as we get more data.
- **Hoeffding's Formula** shows **faster convergence** with larger datasets due to the $\exp(-2m\varepsilon^2)$ term.

```{python convergence-plot}
#| fig-cap: "Hoeffding Bound Convergence Rate"
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Function to calculate Hoeffding bound
def hoeffding_bound(m, epsilon):
    return 2 * np.exp(-2 * m * epsilon**2)

# Dataset sizes
m_values = np.linspace(1, 500, 500)

# Epsilon values
epsilon_values = [0.05, 0.1, 0.2]

# Create the plot
plt.figure(figsize=(12, 3))

# Plotting for different epsilon values
for epsilon in epsilon_values:
    bound_values = hoeffding_bound(m_values, epsilon)
    plt.plot(m_values, bound_values, label=f'Îµ = {epsilon}')

# plt.title('Hoeffding Bound Convergence Rate')
plt.xlabel('Dataset Size (m)')
plt.ylabel('Probability Bound')
plt.yscale('log')  # Log scale for better visualization of exponential decay
plt.legend()
plt.show()
```

---

## Hoeffding's Inequality: Interpretation

- **Meaning**: With more data, training error becomes a better predictor of true error.
- **Practical Implication**: For a fixed model, training performance is a good indicator of unseen data performance, and this improves with dataset size.
- **Limitations**: We usually pick the best model from many, not just one. Hoeffding doesn't account for how complex the model class is.

---

## Union Bound

- **What it does**: Extends bounds like Hoeffding's to work when choosing from **many models** (a hypothesis space $\mathcal{H}$).
- **Main Idea**: Considers the chance that *at least one* model in $\mathcal{H}$ has a large difference between training and true error.

---

## Union Bound: The Maths

- **Expression**[@samir16]:
  $$
  P\left(\sup_{h \in \mathcal{H}} |R(h) - R_{\text{emp}}(h)| > \epsilon \right) \leq \sum_{h \in \mathcal{H}} P\left(|R(h) - R_{\text{emp}}(h)| > \epsilon \right)
  $$
- **Breakdown**:
  - $\sup_{h \in \mathcal{H}}$: Account for the worst-case scenario across all hypotheses.
  - $\sum_{h \in \mathcal{H}}$: Sums up probabilities of large error differences for each model.

---

## Union Bound: Interpretation

- **Larger Model Space**: The more models we consider, the looser the bound becomes.

| **Hypothesis Space Size** | **Bound**      | **Model Capacity** |
| :--------------------- | :--------- | :------------- |
| Small                 | Tighter    | Limited        |
| Large                 | Looser     | Higher         |
: Trade-off: Hypothesis Space vs. Bound & Capacity

---

## Moving Forward

- **Challenge**: Real-world model spaces are often infinite or too large.
- **Solution**: We need ways to measure model complexity that go beyond counting.
- **Next**: Exploring **complexity measures** for more practical generalization bounds.

# Advanced Bounds

## Why Advanced Bounds?

- **Classical Bounds** give us a good starting point, but they can be loose.
- **Goal**: Tighter bounds that better reflect real-world performance.
- **How?**: By measuring model complexity in more sophisticated ways.

---

## VC Dimension

- **Growth Function** ($\Pi_{\mathcal{H}}(m)$): How many ways can a model class ($\mathcal{H}$) label *m* data points?
  - More ways = more complex.
  - For small *m*, $\Pi_{\mathcal{H}}(m)=2^m$. 
- **Shattering**: A model class *shatters* a dataset if it can label it in *every possible way*.

---

## VC Dimension: Definition

- **VC Dimension ($d_{\text{VC}}$)**: The size of the *largest* dataset a model class can shatter.
- **Example**: Linear classifiers in 2D have $d_{\text{VC}} = 3$. They can shatter 3 points but not 4 (in all configurations).

```{python vc-dimension-illustration}
#| fig-cap: "VC Dimension of Linear Classifiers in 2D"
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Create a figure and axes for subplots
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Function to plot points and a linear classifier
def plot_points(ax, points, labels, title):
    ax.scatter(points[:, 0], points[:, 1], c=labels, cmap='viridis', s=100, edgecolors='k')
    ax.set_title(title)
    ax.set_xlabel('Feature 1')
    ax.set_ylabel('Feature 2')

    # Add a linear classifier (for illustration)
    if len(np.unique(labels)) > 1:
        xlim = ax.get_xlim()
        ylim = ax.get_ylim()
        xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 100),
                             np.linspace(ylim[0], ylim[1], 100))
        # Assume a simple linear classifier (y = x) for demonstration
        Z = xx - yy
        ax.contour(xx, yy, Z, levels=[0], colors='red')

# --- 3 Points, Shattered ---
points_3 = np.array([[1, 2], [2, 1], [3, 3]])
labels_3 = np.array([1, -1, 1])  # Example labels that can be separated
plot_points(axes[0], points_3, labels_3, "3 Points, Shattered")

# --- 4 Points, Not Shattered (XOR Configuration) ---
points_4 = np.array([[1, 1], [1, 2], [2, 1], [2, 2]])
labels_4 = np.array([1, -1, -1, 1])  # XOR configuration, cannot be linearly separated
plot_points(axes[1], points_4, labels_4, "4 Points, Not Shattered (XOR)")

# --- 4 Points, Shattered (Convex Configuration) ---
points_4_2 = np.array([[0, 0], [0, 2], [2, 2], [1.5, 0.5]])
labels_4_2 = np.array([1, 1, -1, -1])  # Example labels that can be separated
plot_points(axes[2], points_4_2, labels_4_2 , "4 Points, Shattered (Convex)")

plt.tight_layout()
plt.show()
```

- **Intuition**: Higher $d_{\text{VC}}$ = more complex model.

---

## VC Generalization Bound

- **Formula**[@vapnik95]:
  $$
  R(h) \leq R_{\text{emp}}(h) + \sqrt{\frac{8 d_{\text{VC}} \left(\ln\left(\frac{2m}{d_{\text{VC}}}\right) + 1\right) + 8 \ln\left(\frac{4}{\delta}\right)}{m}}
  $$
  - $R(h)$: True error.
  - $R_{\text{emp}}(h)$: Training error.
  - $d_{\text{VC}}$: VC dimension.
  - $m$: Dataset size.
  - $\delta$: Confidence parameter.

---

## VC Bound: Interpretation

- **Higher VC Dimension**:
  - More complex model, looser bound, higher risk of overfitting.
- **Larger Dataset**:
  - Tighter bound, better generalization.

```{python vc-bound-insights}
#| fig-cap: "Approximation of the VC Generalization Bound"
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set style for prettier plots
sns.set_style('whitegrid')
sns.set_context('talk')

# Function to calculate the VC bound term
def vc_bound_term(d_vc, m, delta=0.05):
    return np.sqrt((8 * d_vc * (np.log((2 * m) / d_vc) + 1) + 8 * np.log(4 / delta)) / m)

# Dataset sizes
m_values = np.linspace(10, 1000, 100)

# VC dimension values
d_vc_values = [1, 5, 10]

# Create the plot
plt.figure(figsize=(10, 2.25))

# Plotting for different VC dimension values
for d_vc in d_vc_values:
    bound_values = vc_bound_term(d_vc, m_values)
    plt.plot(m_values, bound_values, label=f'VC Dimension = {d_vc}')

plt.xlabel('Dataset Size (m)')
plt.ylabel('VC Bound Term')
# plt.title('Impact of VC Dimension and Dataset Size on Generalization Bound')
plt.legend()
plt.show()
```

---

## Distribution-Based Bounds

- **VC theory** often considers the *worst-case* scenario.
- **New Idea**: Use information about the **data distribution** for tighter bounds.
- **Example**: Support Vector Machines (SVMs).
  - **Margin**: Distance from the decision boundary to the nearest data points.
  - Larger margin = better generalization.
- **Benefit**: More realistic bounds reflecting real-world performance.

---

## More Measures of Complexity

- **Why?**: VC dimension can be too pessimistic.
- **Goal**: More nuanced measures, especially for things like neural networks.

| **Measure**                | **Description**                                  | **Key Idea**                                    |
| :---------------------- | :--------------------------------------------- | :------------------------------------------ |
| Covering Numbers   | How many "balls" cover the hypothesis space?      | Smaller = simpler = tighter bounds         |
| Rademacher Complexity | How well can the model fit random noise?          | Lower = less prone to overfitting          |
: Further ways to measure complexity [@bousquet03]

# Conclusions

## Key Takeaways I

- **Generalization** is crucial: We want models to work on **unseen data**, not just the training set.
- **Overfitting** is a risk: More complex models can memorize the training data but fail to generalize.
- **Classical Bounds** highlight the importance of:
  - **Dataset size**: More data leads to better generalization.
  - **Model complexity**: Simpler models (smaller hypothesis spaces) are safer.

---

## Key Takeaways II

- **Advanced Bounds** offer a refined view:
  - **VC Dimension**: Measures a model's ability to shatter data. Higher VC dimension means more complexity.
  - **Distribution-Based**: Leverage data properties for tighter bounds.
- **The Goal**: Balance model expressiveness with the risk of overfitting by controlling complexity and leveraging insights from the data distribution.

---

### References

::: {#refs}
\footnotesize
:::