% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\usetheme[]{Dresden}
\usefonttheme{structurebold}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

%% suppress section title at beginning of new section
% \AtBeginSection{}

%% add lmu logo to title slide
\titlegraphic{\includegraphics[width=0.33\textwidth]{extra/lmu-logo.png}}

%% modify color template to lmu-inspired one
\definecolor{lmugreen}{RGB}{0,136,58}
\setbeamercolor{section in toc}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=lmugreen!80!gray}
\setbeamercolor*{palette primary}{fg=lmugreen!60!black,bg=gray!30!white}
\setbeamercolor*{palette secondary}{fg=lmugreen!70!black,bg=gray!15!white}
\setbeamercolor*{palette tertiary}{bg=lmugreen!80!black,fg=gray!10!white}
\setbeamercolor*{palette quaternary}{fg=lmugreen,bg=gray!5!white}
\setbeamercolor*{sidebar}{fg=lmugreen,bg=gray!15!white}
\setbeamercolor*{palette sidebar primary}{fg=lmugreen!10!black}
\setbeamercolor*{palette sidebar secondary}{fg=white}
\setbeamercolor*{palette sidebar tertiary}{fg=lmugreen!50!black}
\setbeamercolor*{palette sidebar quaternary}{fg=gray!10!white}
\setbeamercolor{titlelike}{parent=palette primary,fg=lmugreen}
\setbeamercolor{frametitle}{bg=lmugreen!10}
\setbeamercolor{frametitle right}{bg=gray!60!white}
\setbeamercolor{title}{bg=lmugreen!10, fg=lmugreen}
\setbeamercolor{item}{fg=lmugreen}
\setbeamercolor{enumerate item}{fg=lmugreen}
\setbeamercolor*{separation line}{}
\setbeamercolor*{fine separation line}{}
\setbeamercolor{structure}{fg=lmugreen}
\setbeamercolor{bibliography entry title}{fg=black}
\setbeamercolor{bibliography entry note}{fg=black}
\setbeamercolor{bibliography entry location}{fg=black}
\setbeamercolor{bibliography entry author}{fg=black}

%% modify beamer template to have slide number on footer
\setbeamertemplate{footline}%{miniframes theme}
{%
  \begin{beamercolorbox}[colsep=1.5pt]{upper separation line foot}
  \end{beamercolorbox}
  \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,%
    leftskip=.3cm,rightskip=.3cm plus1fil]{author in head/foot}%
    \leavevmode{\usebeamerfont{author in head/foot}\insertshortauthor}%
    \hfill%
    {\usebeamerfont{institute in head/foot}\usebeamercolor[fg]{institute in head/foot}\insertshortinstitute}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,%
    leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
    {\usebeamerfont{title in head/foot}\insertshorttitle} \hfill     \insertframenumber/\inserttotalframenumber%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[colsep=1.5pt]{lower separation line foot}
  \end{beamercolorbox}
}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Generalization Bounds},
  pdfauthor={Matteo Mazzarelli},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Generalization Bounds}
\subtitle{Theoretical Foundations of Deep Learning}
\author{Matteo Mazzarelli}
\date{December 17, 2024}

\begin{document}
\frame{\titlepage}


\section{Introduction}\label{introduction}

\begin{frame}{Motivation}
\phantomsection\label{motivation}
\begin{itemize}
\tightlist
\item
  \textbf{Core Question}: How can models trained on limited data perform
  reliably on unseen scenarios?
\item
  \textbf{Generalization} is a fundamental goal in machine learning:
  ensuring models extend their learned patterns to new, unseen data.
\item
  A poorly generalized model risks:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Overfitting}: Performing well on training data but poorly on
    unseen data.
  \item
    \textbf{Underfitting}: Failing to capture the underlying patterns of
    the data.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The Learning Problem}
\phantomsection\label{the-learning-problem}
\begin{itemize}
\tightlist
\item
  \textbf{Supervised Learning}:

  \begin{itemize}
  \tightlist
  \item
    Goal: Learn a function \(f: X \to Y\) mapping inputs \(X\) to
    outputs \(Y\) based on labeled training data.
  \end{itemize}
\item
  \textbf{Key Question}: Can the learned function perform well on unseen
  data?
\item
  \textbf{Generalization}:

  \begin{itemize}
  \tightlist
  \item
    Ability of a model to extend its learning beyond the training data.
  \item
    \textbf{Central Problem} in machine learning: balancing
    \emph{empirical performance} with \emph{future predictions}.
  \end{itemize}
\end{itemize}
\end{frame}

\section{Overfitting}\label{overfitting}

\begin{frame}{Demonstrating Overfitting}
\phantomsection\label{demonstrating-overfitting}
\begin{itemize}
\tightlist
\item
  \textbf{Objective}:

  \begin{itemize}
  \tightlist
  \item
    Show how increasing model complexity (polynomial degree) leads to
    overfitting.
  \end{itemize}
\item
  \textbf{Dataset}:

  \begin{itemize}
  \tightlist
  \item
    Using the scikit-learn \textbf{Diabetes} dataset with a single
    feature (BMI) and a quantitative response variable indicating
    disease progression
    (Target)\textsuperscript{{[}\citeproc{ref-sklearndiabetes11}{1}{]}}.
  \end{itemize}
\item
  \textbf{Approach}:

  \begin{enumerate}
  \tightlist
  \item
    Fit polynomial regression models of varying degrees.
  \item
    Visualize polynomial fits on the training data.
  \item
    Examine the fits' residuals to see how errors behave.
  \item
    Plot training vs.~test errors to highlight overfitting.
  \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{generalization-bounds_presentation_files/figure-beamer/cell-2-output-1.pdf}}

}

\caption{Overfitting Phenomenon in Polynomial Regression}

\end{figure}%
\end{frame}

\begin{frame}{Double Descent}
\phantomsection\label{double-descent}
\begin{itemize}
\tightlist
\item
  Modern machine learning introduces a fascinating twist: \textbf{Double
  Descent}, where increasing model complexity can lead to improved
  generalization after an initial overfitting phase.
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.8\linewidth,height=\textheight,keepaspectratio]{extra/double_descent.png}

}

\caption{Double Descent phenomenon in a Residual Neural
Network\textsuperscript{{[}\citeproc{ref-nakkiran19}{2}{]}}}

\end{figure}%
\end{frame}

\section{Classical Bounds}\label{classical-bounds}

\begin{frame}{Generalization Bounds: Bridging the Gap}
\phantomsection\label{generalization-bounds-bridging-the-gap}
\begin{itemize}
\tightlist
\item
  \textbf{Goal}: Predict a model's performance on \textbf{unseen data}.
\item
  \textbf{Generalization Bounds} provide theoretical guarantees,
  linking:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Generalization Error}: Error on unseen data.
  \item
    \textbf{Empirical Risk}: Error on training data.
  \item
    \textbf{Model Complexity}: Model's flexibility.
  \end{itemize}
\item
  \textbf{Why They Matter}:

  \begin{itemize}
  \tightlist
  \item
    Help understand the trade-offs between:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Accuracy}: How well the model fits the data.
    \item
      \textbf{Complexity}: Ability to model intricate patterns.
    \item
      \textbf{Data Size}: Amount of data needed for reliable learning.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Hoeffding's Inequality: A Foundation}
\phantomsection\label{hoeffdings-inequality-a-foundation}
\begin{itemize}
\tightlist
\item
  \textbf{What it is}: A probabilistic tool that helps estimate how well
  a model will generalize.
\item
  \textbf{Focus}: Quantifies the difference between \textbf{empirical
  risk} (training error) and \textbf{generalization error} (true error)
  for a \emph{single, fixed model}.
\end{itemize}
\end{frame}

\begin{frame}{Hoeffding's Inequality: The Math}
\phantomsection\label{hoeffdings-inequality-the-math}
\begin{itemize}
\item
  \textbf{Formula}: \[
  P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(R(h)\): True error on unseen data.
  \item
    \(R_{\text{emp}}(h)\): Error on training data.
  \item
    \(\varepsilon\): Error tolerance.
  \item
    \(m\): Dataset size.
  \end{itemize}
\item
  \textbf{Interpretation}: The probability of a large difference between
  true error and training error decreases \textbf{exponentially} with:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Larger datasets} (\(m\)).
  \item
    \textbf{Smaller error tolerance} (\(\varepsilon\)).
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Convergence: How Fast Does It Happen?}
\phantomsection\label{convergence-how-fast-does-it-happen}
\begin{itemize}
\tightlist
\item
  \textbf{Rate of Convergence}: How quickly the training error becomes a
  good estimate of the true error as we get more data.
\item
  \textbf{Hoeffding's Formula} shows \textbf{faster convergence} with
  larger datasets due to the \(\exp(-2m\varepsilon^2)\) term.
\end{itemize}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{generalization-bounds_presentation_files/figure-beamer/cell-3-output-1.pdf}}

}

\caption{Hoeffding Bound Convergence Rate}

\end{figure}%
\end{frame}

\begin{frame}{Interpreting Hoeffding's Inequality}
\phantomsection\label{interpreting-hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  \textbf{Meaning}: With more data, training error becomes a better
  predictor of true error.
\item
  \textbf{Practical Implication}: For a fixed model, training
  performance is a good indicator of unseen data performance, and this
  improves with dataset size.
\item
  \textbf{Limitations}: We usually pick the best model from many, not
  just one. Hoeffding doesn't account for how complex the model class
  is.
\end{itemize}
\end{frame}

\begin{frame}{The Union Bound: Handling Multiple Models}
\phantomsection\label{the-union-bound-handling-multiple-models}
\begin{itemize}
\tightlist
\item
  \textbf{What it does}: Extends bounds like Hoeffding's to work when
  choosing from \textbf{many models} (a hypothesis space
  \(\mathcal{H}\)).
\item
  \textbf{Main Idea}: Considers the chance that \emph{at least one}
  model in \(\mathcal{H}\) has a large difference between training and
  true error.
\end{itemize}
\end{frame}

\begin{frame}{Union Bound: The Formula}
\phantomsection\label{union-bound-the-formula}
\begin{itemize}
\tightlist
\item
  \textbf{Expression}: \[
  P\left(\sup_{h \in \mathcal{H}} |R(h) - R_{\text{emp}}(h)| > \epsilon \right) \leq \sum_{h \in \mathcal{H}} P\left(|R(h) - R_{\text{emp}}(h)| > \epsilon \right)
  \]
\item
  \textbf{Breakdown}:

  \begin{itemize}
  \tightlist
  \item
    \(\sup_{h \in \mathcal{H}}\): Account for the worst-case scenario
    across all hypotheses.
  \item
    \(\sum_{h \in \mathcal{H}} P(|R(h) - R_{\text{emp}}(h)| > \epsilon)\):
    Sums up probabilities of large error differences for each model.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Union Bound: Key Implications}
\phantomsection\label{union-bound-key-implications}
\begin{itemize}
\tightlist
\item
  \textbf{Larger Model Space}: The more models we consider, the looser
  the bound becomes.
\end{itemize}

\begin{longtable}[]{@{}lll@{}}
\caption{Trade-off: Hypothesis Space vs.~Bound \&
Capacity}\tabularnewline
\toprule\noalign{}
Hypothesis Space Size & Bound & Model Capacity \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Hypothesis Space Size & Bound & Model Capacity \\
\midrule\noalign{}
\endhead
Small & Tighter & Limited \\
Large & Looser & Higher \\
\bottomrule\noalign{}
\end{longtable}
\end{frame}

\begin{frame}{Moving Forward}
\phantomsection\label{moving-forward}
\begin{itemize}
\tightlist
\item
  \textbf{Challenge}: Real-world model spaces are often infinite or too
  large.
\item
  \textbf{Solution}: We need ways to measure model complexity that go
  beyond counting.
\item
  \textbf{Next}: Exploring \textbf{complexity measures} for more
  practical generalization bounds.
\end{itemize}
\end{frame}

\section{Advanced Bounds}\label{advanced-bounds}

\begin{frame}{Motivation for Advanced Bounds}
\phantomsection\label{motivation-for-advanced-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{Advanced bounds} address a variety of the limitations we have
  outlined by incorporating:

  \begin{itemize}
  \tightlist
  \item
    \textbf{VC Dimension}: A measure of the capacity or expressiveness
    of a hypothesis class. Higher VC dimensions indicate more complex
    models, which may require more data to generalize well.
  \item
    \textbf{Rademacher Complexity}: A data-dependent measure of how well
    a hypothesis class can fit random noise in the training data. It
    captures both the hypothesis class and the specifics of the data
    distribution.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Extending Convergence Rates}:

  \begin{itemize}
  \tightlist
  \item
    Advanced bounds refine the rates of convergence by linking the
    generalization error to:

    \begin{itemize}
    \tightlist
    \item
      The size of the dataset \(m\).
    \item
      The complexity of the hypothesis class (e.g., \textbf{VC
      dimension} or \textbf{Rademacher complexity}).
    \end{itemize}
  \item
    For example, the generalization error is often bounded as: \[
    R(h) - R_{\text{emp}}(h) \leq \mathcal{O}\left(\sqrt{\frac{\text{Complexity}(\mathcal{H})}{m}}\right)
    \]

    \begin{itemize}
    \tightlist
    \item
      Larger datasets \(m\) reduce error, but higher complexity
      increases the required data for a desired level of generalization.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Practical Implications}:

  \begin{itemize}
  \tightlist
  \item
    These bounds provide actionable insights for balancing model
    complexity and dataset size.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Vapnik-Chervonenkis (VC) Theory}
\phantomsection\label{vapnik-chervonenkis-vc-theory}
\begin{itemize}
\tightlist
\item
  \textbf{Growth Function}

  \begin{itemize}
  \tightlist
  \item
    The \textbf{Growth Function} is a measure of the expressiveness of a
    hypothesis space \(\mathcal{H}\).
  \item
    \textbf{Definition}:

    \begin{itemize}
    \tightlist
    \item
      The growth function, \(\Pi_{\mathcal{H}}(m)\), is the maximum
      number of distinct ways a hypothesis space can label \(m\) data
      points.
    \end{itemize}
  \item
    \textbf{Key Idea}:

    \begin{itemize}
    \tightlist
    \item
      A more expressive hypothesis space can label datasets in a greater
      number of ways, indicating higher complexity.
    \end{itemize}
  \item
    \textbf{Growth Behavior}:

    \begin{itemize}
    \tightlist
    \item
      For small \(m\), \(\Pi_{\mathcal{H}}(m) = 2^m\).
    \item
      For larger \(m\), the growth may be limited by the structure of
      \(\mathcal{H}\).
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{VC Dimension}

  \begin{itemize}
  \tightlist
  \item
    The \textbf{VC Dimension} is a scalar value that quantifies the
    capacity of a hypothesis space \(\mathcal{H}\).
  \item
    \textbf{Definition}:

    \begin{itemize}
    \tightlist
    \item
      The VC dimension \(d_{\text{VC}}\) is the size of the largest
      dataset that can be \textbf{shattered} by \(\mathcal{H}\).
    \end{itemize}
  \item
    \textbf{Shattering}:

    \begin{itemize}
    \tightlist
    \item
      A dataset is shattered if every possible labeling of the dataset
      can be perfectly captured by hypotheses in \(\mathcal{H}\).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Examples}:

  \begin{itemize}
  \tightlist
  \item
    A linear classifier in 2D space has a VC dimension of 3 (it can
    shatter any 3 points, but not all configurations of 4 points).
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{VC Generalization Bound}
\phantomsection\label{vc-generalization-bound}
\begin{itemize}
\tightlist
\item
  \textbf{What is the VC Generalization Bound?}

  \begin{itemize}
  \tightlist
  \item
    A theoretical result that connects the \textbf{generalization error}
    with the \textbf{empirical risk}, the \textbf{VC dimension}, and the
    size of the dataset.
  \item
    \textbf{Mathematical Formulation}: \[
    R(h) \leq R_{\text{emp}}(h) + \sqrt{\frac{8 d_{\text{VC}} \left(\ln\left(\frac{2m}{d_{\text{VC}}}\right) + 1\right) + 8 \ln\left(\frac{4}{\delta}\right)}{m}}
    \]

    \begin{itemize}
    \tightlist
    \item
      \(R(h)\): Generalization error.
    \item
      \(R_{\text{emp}}(h)\): Empirical risk.
    \item
      \(d_{\text{VC}}\): VC dimension.
    \item
      \(m\): Dataset size.
    \item
      \(\delta\): Confidence level (\(1 - \delta\) is the probability
      that the bound holds).
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights}
\begin{itemize}
\tightlist
\item
  As \(d_{\text{VC}}\) increases (more complex hypothesis space):

  \begin{itemize}
  \tightlist
  \item
    The bound becomes looser, reflecting a higher risk of overfitting.
  \end{itemize}
\item
  As \(m\) increases (larger dataset size):

  \begin{itemize}
  \tightlist
  \item
    The bound tightens, improving generalization guarantees.
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Summing Up VC Theory}
\phantomsection\label{summing-up-vc-theory}
\begin{itemize}
\tightlist
\item
  \textbf{Expressiveness vs.~Generalization}:

  \begin{itemize}
  \tightlist
  \item
    The VC dimension captures the \textbf{expressiveness} of a
    hypothesis space:

    \begin{itemize}
    \tightlist
    \item
      Higher \(d_{\text{VC}}\): More complex, more expressive.
    \end{itemize}
  \item
    A balance is required to avoid overfitting (high complexity) or
    underfitting (low complexity).
  \end{itemize}
\item
  \textbf{Implications for Learning}:

  \begin{itemize}
  \tightlist
  \item
    The VC dimension helps understand:

    \begin{itemize}
    \tightlist
    \item
      Why simpler models often generalize better.
    \item
      Why increasing data size improves generalization, especially for
      complex models.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Foundation for Algorithm Design}:

  \begin{itemize}
  \tightlist
  \item
    VC theory guides the development of learning algorithms by
    quantifying the trade-offs between hypothesis complexity, data size,
    and generalization performance.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Distribution-Based Bounds}
\phantomsection\label{distribution-based-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{From General Bounds to Data-Driven Insights}:

  \begin{itemize}
  \tightlist
  \item
    Generalization bounds like Hoeffding's inequality and VC bounds rely
    on worst-case scenarios.
  \end{itemize}
\item
  \textbf{Distribution-Based Bounds}:

  \begin{itemize}
  \tightlist
  \item
    Leverage specific properties of the data distribution to achieve
    \textbf{tighter bounds}.
  \item
    Exploit \textbf{data structure} to understand how well a model
    generalizes in practice.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Example: Support Vector Machines (SVMs)}
\phantomsection\label{example-support-vector-machines-svms}
\begin{itemize}
\tightlist
\item
  \textbf{SVMs and Margin-Based Bounds}:

  \begin{itemize}
  \tightlist
  \item
    Support Vector Machines (SVMs) introduce the concept of a
    \textbf{margin}, the distance between the decision boundary and the
    nearest data points.
  \item
    \textbf{Intuition}:

    \begin{itemize}
    \tightlist
    \item
      A larger margin indicates better separation between classes,
      leading to better generalization.
    \end{itemize}
  \item
    \textbf{Margin-Based Generalization Bounds}:

    \begin{itemize}
    \tightlist
    \item
      Generalization error decreases as the margin increases, even for
      infinite hypothesis spaces.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Alternative Capacity Measures}
\phantomsection\label{alternative-capacity-measures}
\begin{itemize}
\tightlist
\item
  \textbf{Why Explore Alternative Measures?}

  \begin{itemize}
  \tightlist
  \item
    VC dimension assumes worst-case datasets, often leading to overly
    conservative bounds.
  \item
    Alternative measures provide more nuanced insights into hypothesis
    space complexity, especially for modern machine learning models like
    neural networks.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Examples of Alternative Measures}

  \begin{enumerate}
  \tightlist
  \item
    \textbf{Covering Numbers}: The minimum number of small ``balls''
    needed to cover the hypothesis space under a certain metric. Smaller
    covering numbers indicate a simpler hypothesis space, leading to
    tighter generalization bounds.
  \item
    \textbf{Rademacher Complexity}: Measures the ability of a hypothesis
    class to fit random noise. A lower Rademacher complexity indicates
    that the hypothesis space is less prone to overfitting.
  \end{enumerate}
\item
  \textbf{Next Steps}:

  \begin{itemize}
  \tightlist
  \item
    We want to explore how these theoretical tools are applied to modern
    machine learning methods.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights-1}
\begin{itemize}
\tightlist
\item
  These measures allow for tighter, data-adaptive generalization bounds,
  particularly useful for complex or large-scale models.
\item
  There's no one-size-fits-all measure. The choice of capacity measure
  depends on:

  \begin{itemize}
  \tightlist
  \item
    The hypothesis space.
  \item
    The structure of the data.
  \item
    The learning algorithm.
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\section{Conclusions}\label{conclusions}

\begin{frame}
\begin{block}{References}
\phantomsection\label{references}
\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\footnotesize

\bibitem[\citeproctext]{ref-sklearndiabetes11}
1. Pedregosa F., Varoquaux G., \& et al. (2011). \emph{Scikit-learn:
Machine learning in python, diabetes dataset}.
\url{https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_diabetes.html}

\bibitem[\citeproctext]{ref-nakkiran19}
2. Nakkiran P., Kaplun G., \& et al. (2019). \emph{Deep double descent:
Where bigger models and more data hurt}.
\url{https://arxiv.org/abs/1912.02292}

\bibitem[\citeproctext]{ref-bousquet03}
3. Bousquet O., Boucheron S., \& Lugosi G. (2003). Introduction to
statistical learning theory. \emph{Advanced Lectures on Machine
Learning}.

\bibitem[\citeproctext]{ref-samir16}
4. Samir M. (2016). \emph{A gentle introduction to statistical learning
theory}. \url{https://mostafa-samir.github.io/ml-theory-pt2/}.

\bibitem[\citeproctext]{ref-vapnik98}
5. Vapnik V. N. (1995). \emph{The nature of statistical learning
theory}. Springer.

\bibitem[\citeproctext]{ref-mohri12}
6. Mohri M., Rostamizadeh A., \& Talwalkar A. (2012). \emph{Foundations
of machine learning}. MIT Press.

\end{CSLReferences}
\end{block}
\end{frame}




\end{document}
