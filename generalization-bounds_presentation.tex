% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\usetheme[]{Dresden}
\usefonttheme{structurebold}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother

%% suppress section title at beginning of new section
% \AtBeginSection{}

%% add lmu logo to title slide
\titlegraphic{\includegraphics[width=0.33\textwidth]{lmu-logo.png}}

%% modify color template to lmu-inspired one
\definecolor{lmugreen}{RGB}{0,136,58}
\setbeamercolor{section in toc}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=lmugreen!80!gray}
\setbeamercolor*{palette primary}{fg=lmugreen!60!black,bg=gray!30!white}
\setbeamercolor*{palette secondary}{fg=lmugreen!70!black,bg=gray!15!white}
\setbeamercolor*{palette tertiary}{bg=lmugreen!80!black,fg=gray!10!white}
\setbeamercolor*{palette quaternary}{fg=lmugreen,bg=gray!5!white}
\setbeamercolor*{sidebar}{fg=lmugreen,bg=gray!15!white}
\setbeamercolor*{palette sidebar primary}{fg=lmugreen!10!black}
\setbeamercolor*{palette sidebar secondary}{fg=white}
\setbeamercolor*{palette sidebar tertiary}{fg=lmugreen!50!black}
\setbeamercolor*{palette sidebar quaternary}{fg=gray!10!white}
\setbeamercolor{titlelike}{parent=palette primary,fg=lmugreen}
\setbeamercolor{frametitle}{bg=lmugreen!10}
\setbeamercolor{frametitle right}{bg=gray!60!white}
\setbeamercolor{title}{bg=lmugreen!10, fg=lmugreen}
\setbeamercolor{item}{fg=lmugreen}
\setbeamercolor{enumerate item}{fg=lmugreen}
\setbeamercolor*{separation line}{}
\setbeamercolor*{fine separation line}{}

%% modify beamer template to have slide number on footer
\setbeamertemplate{footline}%{miniframes theme}
{%
  \begin{beamercolorbox}[colsep=1.5pt]{upper separation line foot}
  \end{beamercolorbox}
  \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,%
    leftskip=.3cm,rightskip=.3cm plus1fil]{author in head/foot}%
    \leavevmode{\usebeamerfont{author in head/foot}\insertshortauthor}%
    \hfill%
    {\usebeamerfont{institute in head/foot}\usebeamercolor[fg]{institute in head/foot}\insertshortinstitute}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,%
    leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
    {\usebeamerfont{title in head/foot}\insertshorttitle} \hfill     \insertframenumber/\inserttotalframenumber%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[colsep=1.5pt]{lower separation line foot}
  \end{beamercolorbox}
}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Generalization Bounds},
  pdfauthor={Matteo Mazzarelli},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Generalization Bounds}
\subtitle{Theoretical Foundations of Deep Learning}
\author{Matteo Mazzarelli}
\date{December 17, 2024}

\begin{document}
\frame{\titlepage}


\section{Introduction}\label{introduction}

\begin{frame}{Motivation}
\phantomsection\label{motivation}
\begin{itemize}
\tightlist
\item
  \textbf{Core Question}: How can models trained on limited data perform
  reliably on unseen scenarios?
\item
  \textbf{Generalization} is a fundamental goal in machine learning:
  ensuring models extend their learned patterns to new, unseen data.
\item
  A poorly generalized model risks:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Overfitting}: Performing well on training data but poorly on
    unseen data.
  \item
    \textbf{Underfitting}: Failing to capture the underlying patterns of
    the data.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The Learning Problem}
\phantomsection\label{the-learning-problem}
\begin{itemize}
\tightlist
\item
  \textbf{Supervised Learning}:

  \begin{itemize}
  \tightlist
  \item
    Goal: Learn a function \(f: X \to Y\) mapping inputs \(X\) to
    outputs \(Y\) based on labeled training data.
  \end{itemize}
\item
  \textbf{Key Question}: Can the learned function perform well on unseen
  data?
\item
  \textbf{Generalization}:

  \begin{itemize}
  \tightlist
  \item
    Ability of a model to extend its learning beyond the training data.
  \item
    \textbf{Central Problem} in machine learning: balancing
    \emph{empirical performance} with \emph{future predictions}.
  \end{itemize}
\end{itemize}
\end{frame}

\section{Overfitting}\label{overfitting}

\begin{frame}[fragile]{Simulating Overfitting}
\phantomsection\label{simulating-overfitting}
\begin{itemize}
\tightlist
\item
  \textbf{Objective}:

  \begin{itemize}
  \tightlist
  \item
    Visualize the impact of model complexity on overfitting in a linear
    regression model.
  \end{itemize}
\item
  \textbf{Dataset}

  \begin{itemize}
  \tightlist
  \item
    The experiment uses the \textbf{Boston Housing dataset}, where the
    target variable is \texttt{medv} (median value of owner-occupied
    homes), and the features represent housing characteristics.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Experimental Setup}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Model Complexity}:

    \begin{itemize}
    \tightlist
    \item
      Complexity is defined by the number of features included in the
      model.
    \item
      Additional random features are generated to simulate increasing
      complexity beyond the real features in the dataset.
    \end{itemize}
  \item
    \textbf{Train-Test Split}:

    \begin{itemize}
    \tightlist
    \item
      The dataset is split into 70\% training and 30\% test data.
    \end{itemize}
  \item
    \textbf{Range of Complexity}:

    \begin{itemize}
    \tightlist
    \item
      Models are trained with 1 to 200 features. Beyond the actual
      features in the dataset, random noise features are added
      incrementally.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Procedure}: For each level of complexity:

  \begin{enumerate}
  \tightlist
  \item
    A subset of features (real and random) is used to train a linear
    regression model.
  \item
    Predictions are made on both the training and test datasets.
  \item
    Mean Squared Errors (MSE) are calculated for both datasets.
  \end{enumerate}
\item
  \textbf{Results}:

  \begin{itemize}
  \tightlist
  \item
    Training error decreases consistently as model complexity increases.
  \item
    Test error initially decreases but then increases, demonstrating the
    overfitting phenomenon.
  \end{itemize}
\item
  \textbf{Visualization}:

  \begin{itemize}
  \tightlist
  \item
    A line plot shows the relationship between model complexity (number
    of features) and mean squared error for both the training and test
    datasets.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}[H]

{\centering \includegraphics[width=0.9\linewidth,height=\textheight,keepaspectratio]{generalization-bounds_presentation_files/figure-beamer/overfitting-plot-1.pdf}

}

\caption{Overfitting Phenomenon in Linear Regression}

\end{figure}%

\begin{itemize}
\tightlist
\item
  \textbf{Highlights}:

  \begin{itemize}
  \tightlist
  \item
    The \textbf{bias-variance tradeoff}.
  \item
    The point where overfitting begins, indicated by the divergence of
    training and test errors.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights}
\begin{itemize}
\tightlist
\item
  Increasing model complexity without consideration of the underlying
  data structure can lead to overfitting.
\item
  Simple models that focus on the true underlying pattern often
  generalize better to unseen data.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Double Descent}
\phantomsection\label{double-descent}
\begin{itemize}
\tightlist
\item
  However, modern machine learning introduces a fascinating twist:
  \textbf{Double Descent}, where increasing model complexity can
  sometimes lead to improved generalization after an initial overfitting
  phase.
\item
  Unlike traditional models where increasing complexity leads to
  overfitting, further increasing the complexity (e.g., using
  overparameterized neural networks) can eventually reduce the
  generalization error after an initial peak.
\item
  This challenges the classical view of overfitting and highlights the
  complex relationship between model complexity and generalization in
  modern machine learning.
\end{itemize}
\end{frame}

\section{Classical Bounds}\label{classical-bounds}

\begin{frame}{Introducing Generalization Bounds}
\phantomsection\label{introducing-generalization-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{What Are Generalization Bounds?}

  \begin{itemize}
  \tightlist
  \item
    Theoretical tools offering guarantees about a model's performance on
    unseen data.
  \item
    Relate:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Generalization Error}: How well the model performs on
      unseen data.
    \item
      \textbf{Empirical Risk}: Performance observed on training data.
    \item
      \textbf{Model Complexity}: How expressive the model is.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Purpose}:

  \begin{itemize}
  \tightlist
  \item
    Provide insights into the trade-offs between:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Model Accuracy}: How well the model captures the data
      patterns.
    \item
      \textbf{Model Complexity}: The expressiveness of the model and its
      ability to fit intricate patterns.
    \item
      \textbf{Training Data Size}: How much data is required to achieve
      reliable generalization.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Hoeffding's Inequality: A Starting Point}
\phantomsection\label{hoeffdings-inequality-a-starting-point}
\begin{itemize}
\tightlist
\item
  \textbf{What is Hoeffding's Inequality?}

  \begin{itemize}
  \tightlist
  \item
    A fundamental result in probability theory used to bound the
    difference between the \textbf{empirical risk} and the
    \textbf{generalization error} for a fixed hypothesis.
  \item
    Provides a way to measure how closely a model's performance on
    training data reflects its performance on unseen data.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Mathematical Formulation of Hoeffding's Inequality}
\phantomsection\label{mathematical-formulation-of-hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  \textbf{Hoeffding's Inequality}: \[
  P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(R(h)\): Generalization error (true performance on unseen data).
  \item
    \(R_{\text{emp}}(h)\): Empirical risk (error on training data).
  \item
    \(\varepsilon\): A small positive value (tolerance).
  \item
    \(m\): Size of the dataset.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights-1}
\begin{itemize}
\tightlist
\item
  The probability that the generalization error \(R(h)\) deviates
  significantly from the empirical risk \(R_{\text{emp}}(h)\) decreases
  \textbf{exponentially} with:

  \begin{itemize}
  \tightlist
  \item
    Larger dataset size \(m\).
  \item
    Smaller tolerance \(\varepsilon\).
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Rates of Convergence}
\phantomsection\label{rates-of-convergence}
\begin{itemize}
\tightlist
\item
  \textbf{What Are Rates of Convergence?}

  \begin{itemize}
  \tightlist
  \item
    Quantify how quickly the generalization error approaches the
    empirical risk as the dataset size \(m\) grows.
  \item
    Provide a guideline for determining the dataset size needed to
    achieve a desired level of generalization.
  \item
    In Hoeffding's inequality: \[
    P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
    \]

    \begin{itemize}
    \tightlist
    \item
      The \textbf{exponential term} \(\exp(-2m\varepsilon^2)\) shows
      that the convergence is faster with larger datasets.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Key Factors}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Dataset Size (\(m\))}: Larger datasets reduce the gap
    between \(R(h)\) and \(R_{\text{emp}}(h)\) more quickly.
  \item
    \textbf{Tolerance (\(\varepsilon\))}: Smaller tolerances require
    larger datasets for the same level of confidence.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Interpretation of Hoeffding's Inequality}
\phantomsection\label{interpretation-of-hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  \textbf{What Does It Mean?}

  \begin{itemize}
  \tightlist
  \item
    As the dataset size (\(m\)) increases, the empirical risk becomes a
    more reliable indicator of the generalization error.
  \item
    For a fixed hypothesis, we can be confident that the performance
    observed on training data is close to what can be expected on unseen
    data.
  \item
    The \textbf{rate of convergence} shows how quickly this reliability
    improves as \(m\) grows.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights-2}
\begin{itemize}
\tightlist
\item
  Hoeffding's inequality gives a \textbf{quantitative guarantee} about
  the relationship between training performance and unseen data
  performance.
\item
  Understanding convergence rates helps in planning how much data is
  needed for robust generalization.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Limitations of Hoeffding's Inequality}
\phantomsection\label{limitations-of-hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  \textbf{Beyond Fixed Hypotheses}:

  \begin{itemize}
  \tightlist
  \item
    Hoeffding's inequality assumes a single, fixed hypothesis.
  \item
    In practice, machine learning involves selecting the best hypothesis
    from a \textbf{large hypothesis class} \(\mathcal{H}\), increasing
    the risk of overfitting.
  \end{itemize}
\item
  \textbf{Need for Complexity-Aware Bounds}:

  \begin{itemize}
  \tightlist
  \item
    Simple bounds like Hoeffding's do not consider the complexity of the
    hypothesis class, which influences generalization.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The Union Bound}
\phantomsection\label{the-union-bound}
\begin{itemize}
\tightlist
\item
  \textbf{What is the Union Bound?}

  \begin{itemize}
  \tightlist
  \item
    A probability tool used to extend bounds like Hoeffding's inequality
    to apply across an entire hypothesis space \(\mathcal{H}\).
  \item
    Helps estimate the probability that \textbf{at least one hypothesis}
    in \(\mathcal{H}\) has a large generalization gap.
  \end{itemize}
\item
  \textbf{Key Idea}:

  \begin{itemize}
  \tightlist
  \item
    Instead of considering a single fixed hypothesis, the Union Bound
    aggregates the probabilities of generalization gaps over all
    hypotheses in \(\mathcal{H}\).
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Formalization of The Union Bound}
\phantomsection\label{formalization-of-the-union-bound}
\begin{itemize}
\tightlist
\item
  \textbf{Mathematical Expression}: \[
  P\left(\sup_{h \in \mathcal{H}} |R(h) - R_{\text{emp}}(h)| > \epsilon \right) \leq \sum_{h \in \mathcal{H}} P\left(|R(h) - R_{\text{emp}}(h)| > \epsilon \right)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(\sup_{h \in \mathcal{H}}\): The supremum ensures we account for
    the worst-case scenario across all hypotheses.
  \item
    \(P(|R(h) - R_{\text{emp}}(h)| > \epsilon)\): The probability of a
    significant generalization gap for each hypothesis.
  \end{itemize}
\item
  \textbf{How It Works}:

  \begin{itemize}
  \tightlist
  \item
    By summing up the probabilities for all hypotheses, the Union Bound
    provides a way to analyze the worst-case scenario over the
    hypothesis space.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Implications of The Union Bound}
\phantomsection\label{implications-of-the-union-bound}
\begin{itemize}
\tightlist
\item
  \textbf{Impact of Hypothesis Space Size}:

  \begin{itemize}
  \tightlist
  \item
    The bound depends directly on the \textbf{size of the hypothesis
    space} \(|\mathcal{H}|\).
  \item
    Larger hypothesis spaces increase the sum, making the bound looser.
  \end{itemize}
\item
  \textbf{Takeaway}:

  \begin{itemize}
  \tightlist
  \item
    The Union Bound highlights a trade-off:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Small hypothesis space}: Tighter bounds, but limited model
      capacity.
    \item
      \textbf{Large hypothesis space}: Higher capacity, but risk of
      overfitting and looser bounds.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Transition to Advanced Bounds}
\phantomsection\label{transition-to-advanced-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{Connection to Practical Learning}:

  \begin{itemize}
  \tightlist
  \item
    In practice, hypothesis spaces are often infinite or too large to
    enumerate explicitly. This motivates the need for alternative ways
    to measure hypothesis complexity.
  \end{itemize}
\item
  \textbf{From Simple to Sophisticated}:

  \begin{itemize}
  \tightlist
  \item
    The Union Bound provides a conceptual basis for understanding how
    hypothesis space size affects generalization.
  \item
    Next, we delve into \textbf{complexity measures} that allow us to
    extend generalization bounds to more practical, infinite hypothesis
    spaces.
  \end{itemize}
\end{itemize}
\end{frame}

\section{Advanced Bounds}\label{advanced-bounds}

\begin{frame}{Motivation for Advanced Bounds}
\phantomsection\label{motivation-for-advanced-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{Advanced bounds} address a variety of the limitations we have
  outlined by incorporating:

  \begin{itemize}
  \tightlist
  \item
    \textbf{VC Dimension}: A measure of the capacity or expressiveness
    of a hypothesis class. Higher VC dimensions indicate more complex
    models, which may require more data to generalize well.
  \item
    \textbf{Rademacher Complexity}: A data-dependent measure of how well
    a hypothesis class can fit random noise in the training data. It
    captures both the hypothesis class and the specifics of the data
    distribution.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Extending Convergence Rates}:

  \begin{itemize}
  \tightlist
  \item
    Advanced bounds refine the rates of convergence by linking the
    generalization error to:

    \begin{itemize}
    \tightlist
    \item
      The size of the dataset \(m\).
    \item
      The complexity of the hypothesis class (e.g., \textbf{VC
      dimension} or \textbf{Rademacher complexity}).
    \end{itemize}
  \item
    For example, the generalization error is often bounded as: \[
    R(h) - R_{\text{emp}}(h) \leq \mathcal{O}\left(\sqrt{\frac{\text{Complexity}(\mathcal{H})}{m}}\right)
    \]

    \begin{itemize}
    \tightlist
    \item
      Larger datasets \(m\) reduce error, but higher complexity
      increases the required data for a desired level of generalization.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Practical Implications}:

  \begin{itemize}
  \tightlist
  \item
    These bounds provide actionable insights for balancing model
    complexity and dataset size.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Vapnik-Chervonenkis (VC) Theory}
\phantomsection\label{vapnik-chervonenkis-vc-theory}
\begin{itemize}
\tightlist
\item
  \textbf{Growth Function}

  \begin{itemize}
  \tightlist
  \item
    The \textbf{Growth Function} is a measure of the expressiveness of a
    hypothesis space \(\mathcal{H}\).
  \item
    \textbf{Definition}:

    \begin{itemize}
    \tightlist
    \item
      The growth function, \(\Pi_{\mathcal{H}}(m)\), is the maximum
      number of distinct ways a hypothesis space can label \(m\) data
      points.
    \end{itemize}
  \item
    \textbf{Key Idea}:

    \begin{itemize}
    \tightlist
    \item
      A more expressive hypothesis space can label datasets in a greater
      number of ways, indicating higher complexity.
    \end{itemize}
  \item
    \textbf{Growth Behavior}:

    \begin{itemize}
    \tightlist
    \item
      For small \(m\), \(\Pi_{\mathcal{H}}(m) = 2^m\).
    \item
      For larger \(m\), the growth may be limited by the structure of
      \(\mathcal{H}\).
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{VC Dimension}

  \begin{itemize}
  \tightlist
  \item
    The \textbf{VC Dimension} is a scalar value that quantifies the
    capacity of a hypothesis space \(\mathcal{H}\).
  \item
    \textbf{Definition}:

    \begin{itemize}
    \tightlist
    \item
      The VC dimension \(d_{\text{VC}}\) is the size of the largest
      dataset that can be \textbf{shattered} by \(\mathcal{H}\).
    \end{itemize}
  \item
    \textbf{Shattering}:

    \begin{itemize}
    \tightlist
    \item
      A dataset is shattered if every possible labeling of the dataset
      can be perfectly captured by hypotheses in \(\mathcal{H}\).
    \end{itemize}
  \end{itemize}
\item
  \textbf{Examples}:

  \begin{itemize}
  \tightlist
  \item
    A linear classifier in 2D space has a VC dimension of 3 (it can
    shatter any 3 points, but not all configurations of 4 points).
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{VC Generalization Bound}
\phantomsection\label{vc-generalization-bound}
\begin{itemize}
\tightlist
\item
  \textbf{What is the VC Generalization Bound?}

  \begin{itemize}
  \tightlist
  \item
    A theoretical result that connects the \textbf{generalization error}
    with the \textbf{empirical risk}, the \textbf{VC dimension}, and the
    size of the dataset.
  \item
    \textbf{Mathematical Formulation}: \[
    R(h) \leq R_{\text{emp}}(h) + \sqrt{\frac{8 d_{\text{VC}} \left(\ln\left(\frac{2m}{d_{\text{VC}}}\right) + 1\right) + 8 \ln\left(\frac{4}{\delta}\right)}{m}}
    \]

    \begin{itemize}
    \tightlist
    \item
      \(R(h)\): Generalization error.
    \item
      \(R_{\text{emp}}(h)\): Empirical risk.
    \item
      \(d_{\text{VC}}\): VC dimension.
    \item
      \(m\): Dataset size.
    \item
      \(\delta\): Confidence level (\(1 - \delta\) is the probability
      that the bound holds).
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights-3}
\begin{itemize}
\tightlist
\item
  As \(d_{\text{VC}}\) increases (more complex hypothesis space):

  \begin{itemize}
  \tightlist
  \item
    The bound becomes looser, reflecting a higher risk of overfitting.
  \end{itemize}
\item
  As \(m\) increases (larger dataset size):

  \begin{itemize}
  \tightlist
  \item
    The bound tightens, improving generalization guarantees.
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Summing Up VC Theory}
\phantomsection\label{summing-up-vc-theory}
\begin{itemize}
\tightlist
\item
  \textbf{Expressiveness vs.~Generalization}:

  \begin{itemize}
  \tightlist
  \item
    The VC dimension captures the \textbf{expressiveness} of a
    hypothesis space:

    \begin{itemize}
    \tightlist
    \item
      Higher \(d_{\text{VC}}\): More complex, more expressive.
    \end{itemize}
  \item
    A balance is required to avoid overfitting (high complexity) or
    underfitting (low complexity).
  \end{itemize}
\item
  \textbf{Implications for Learning}:

  \begin{itemize}
  \tightlist
  \item
    The VC dimension helps understand:

    \begin{itemize}
    \tightlist
    \item
      Why simpler models often generalize better.
    \item
      Why increasing data size improves generalization, especially for
      complex models.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Foundation for Algorithm Design}:

  \begin{itemize}
  \tightlist
  \item
    VC theory guides the development of learning algorithms by
    quantifying the trade-offs between hypothesis complexity, data size,
    and generalization performance.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Distribution-Based Bounds}
\phantomsection\label{distribution-based-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{From General Bounds to Data-Driven Insights}:

  \begin{itemize}
  \tightlist
  \item
    Generalization bounds like Hoeffding's inequality and VC bounds rely
    on worst-case scenarios.
  \end{itemize}
\item
  \textbf{Distribution-Based Bounds}:

  \begin{itemize}
  \tightlist
  \item
    Leverage specific properties of the data distribution to achieve
    \textbf{tighter bounds}.
  \item
    Exploit \textbf{data structure} to understand how well a model
    generalizes in practice.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Example: Support Vector Machines (SVMs)}
\phantomsection\label{example-support-vector-machines-svms}
\begin{itemize}
\tightlist
\item
  \textbf{SVMs and Margin-Based Bounds}:

  \begin{itemize}
  \tightlist
  \item
    Support Vector Machines (SVMs) introduce the concept of a
    \textbf{margin}, the distance between the decision boundary and the
    nearest data points.
  \item
    \textbf{Intuition}:

    \begin{itemize}
    \tightlist
    \item
      A larger margin indicates better separation between classes,
      leading to better generalization.
    \end{itemize}
  \item
    \textbf{Margin-Based Generalization Bounds}:

    \begin{itemize}
    \tightlist
    \item
      Generalization error decreases as the margin increases, even for
      infinite hypothesis spaces.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Alternative Capacity Measures}
\phantomsection\label{alternative-capacity-measures}
\begin{itemize}
\tightlist
\item
  \textbf{Why Explore Alternative Measures?}

  \begin{itemize}
  \tightlist
  \item
    VC dimension assumes worst-case datasets, often leading to overly
    conservative bounds.
  \item
    Alternative measures provide more nuanced insights into hypothesis
    space complexity, especially for modern machine learning models like
    neural networks.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Examples of Alternative Measures}

  \begin{enumerate}
  \tightlist
  \item
    \textbf{Covering Numbers}: The minimum number of small ``balls''
    needed to cover the hypothesis space under a certain metric. Smaller
    covering numbers indicate a simpler hypothesis space, leading to
    tighter generalization bounds.
  \item
    \textbf{Rademacher Complexity}: Measures the ability of a hypothesis
    class to fit random noise. A lower Rademacher complexity indicates
    that the hypothesis space is less prone to overfitting.
  \end{enumerate}
\item
  \textbf{Next Steps}:

  \begin{itemize}
  \tightlist
  \item
    We want to explore how these theoretical tools are applied to modern
    machine learning methods.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights-4}
\begin{itemize}
\tightlist
\item
  These measures allow for tighter, data-adaptive generalization bounds,
  particularly useful for complex or large-scale models.
\item
  There's no one-size-fits-all measure. The choice of capacity measure
  depends on:

  \begin{itemize}
  \tightlist
  \item
    The hypothesis space.
  \item
    The structure of the data.
  \item
    The learning algorithm.
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\section{Conclusion}\label{conclusion}




\end{document}
