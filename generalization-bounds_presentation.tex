% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\usetheme[]{Dresden}
\usefonttheme{structurebold}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\AtBeginSection{}
\titlegraphic{\includegraphics[width=0.33\textwidth]{lmu-logo.png}}
\definecolor{lmugreen}{RGB}{0,136,58}
\setbeamercolor{section in toc}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=lmugreen!80!gray}
\setbeamercolor*{palette primary}{fg=lmugreen!60!black,bg=gray!30!white}
\setbeamercolor*{palette secondary}{fg=lmugreen!70!black,bg=gray!15!white}
\setbeamercolor*{palette tertiary}{bg=lmugreen!80!black,fg=gray!10!white}
\setbeamercolor*{palette quaternary}{fg=lmugreen,bg=gray!5!white}
\setbeamercolor*{sidebar}{fg=lmugreen,bg=gray!15!white}
\setbeamercolor*{palette sidebar primary}{fg=lmugreen!10!black}
\setbeamercolor*{palette sidebar secondary}{fg=white}
\setbeamercolor*{palette sidebar tertiary}{fg=lmugreen!50!black}
\setbeamercolor*{palette sidebar quaternary}{fg=gray!10!white}
\setbeamercolor{titlelike}{parent=palette primary,fg=lmugreen}
\setbeamercolor{frametitle}{bg=lmugreen!10}
\setbeamercolor{frametitle right}{bg=gray!60!white}
\setbeamercolor{title}{bg=lmugreen!10, fg=lmugreen}
\setbeamercolor{item}{fg=lmugreen}
\setbeamercolor{enumerate item}{fg=lmugreen}
\setbeamercolor*{separation line}{}
\setbeamercolor*{fine separation line}{}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Generalization Bounds},
  pdfauthor={Matteo Mazzarelli},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Generalization Bounds}
\subtitle{Theoretical Foundations of Deep Learning}
\author{Matteo Mazzarelli}
\date{December 17, 2024}

\begin{document}
\frame{\titlepage}


\section{Introduction}\label{introduction}

\begin{frame}{What is Machine Learning?}
\phantomsection\label{what-is-machine-learning}
\begin{itemize}
\tightlist
\item
  Machine learning is the process of learning from data to make
  predictions or decisions. {[}1{]}
\end{itemize}
\end{frame}

\begin{frame}{Supervised Learning}
\phantomsection\label{supervised-learning}
\begin{itemize}
\tightlist
\item
  We focus on supervised learning where the data consists of
  input-output pairs, called features (\(x_i\)) and labels (\(y_i\)).
  {[}1-3{]}
\item
  The goal is to infer \(y_i\) from \(x_i\). {[}4{]}
\end{itemize}
\end{frame}

\begin{frame}{The Learning Problem}
\phantomsection\label{the-learning-problem}
\begin{itemize}
\tightlist
\item
  We have a dataset of observations
  \(S = \{(x_1, y_1), â€¦, (x_m, y_m)\}\). {[}4{]}
\item
  We wish to learn how to infer the value of \(y_i\) given \(x_i\).
  {[}4{]}
\end{itemize}
\end{frame}

\section{Formalizing the Learning
Problem}\label{formalizing-the-learning-problem}

\begin{frame}{Statistical Model}
\phantomsection\label{statistical-model}
\begin{itemize}
\tightlist
\item
  We assume the values of \((x_i, y_i)\) in the dataset are a random
  sample from a larger population. {[}5{]}
\item
  The values of \(x_i\) and \(y_i\) are realizations of two random
  variables \(X\) and \(Y\) with probability distributions \(P_X\) and
  \(P_Y\) respectively. {[}5{]}
\end{itemize}
\end{frame}

\begin{frame}{Joint Distribution}
\phantomsection\label{joint-distribution}
\begin{itemize}
\tightlist
\item
  There is a relation between the features and the labels. {[}6{]}
\item
  The value of \(Y\) is conditioned on the value of \(X\). {[}6{]}
\item
  This is expressed by the conditional probability \(P(Y|X)\). {[}6{]}
\item
  We can compress \(P_X\) and \(P_Y\) into a single joint distribution
  \(P(X, Y) = P(X)P(Y|X)\). {[}6{]}
\end{itemize}
\end{frame}

\begin{frame}{Target Function}
\phantomsection\label{target-function}
\begin{itemize}
\tightlist
\item
  The \textbf{target function} is \(f(X) = \mathbb{E}[Y|X]\). {[}7{]}
\item
  It represents the expected value of the label \(Y\) given the features
  \(X\). {[}7{]}
\item
  It becomes the target of the machine learning process. {[}7{]}
\item
  The goal is to estimate this function \(f\). {[}7{]}
\end{itemize}
\end{frame}

\begin{frame}{Hypothesis and Hypothesis Space}
\phantomsection\label{hypothesis-and-hypothesis-space}
\begin{itemize}
\tightlist
\item
  A \textbf{hypothesis}, denoted by \(h\), attempts to estimate the
  target function \(f\). {[}7{]}
\item
  The \textbf{hypothesis space}, denoted by \(\mathcal{H}\), is the set
  of possible functions considered for \(h\). {[}7, 8{]}
\end{itemize}
\end{frame}

\section{The Need for Generalization}\label{the-need-for-generalization}

\begin{frame}{Empirical Risk}
\phantomsection\label{empirical-risk}
\begin{itemize}
\tightlist
\item
  The \textbf{empirical risk}, or training error, \(R_{emp}(h)\), is the
  average loss of a hypothesis \(h\) on the training data. {[}9{]}
\item
  It can be calculated using a loss function that quantifies the
  difference between the predicted and actual labels. {[}9{]}
\end{itemize}
\end{frame}

\begin{frame}{Overfitting}
\phantomsection\label{overfitting}
\begin{itemize}
\tightlist
\item
  Simply minimizing empirical risk can lead to \textbf{overfitting}.
  {[}10{]}
\item
  An overfit model performs well on the training data, but poorly on
  unseen data. {[}10, 11{]}
\item
  This occurs when the model learns the specific details of the training
  data instead of the underlying patterns. {[}10{]}
\end{itemize}
\end{frame}

\begin{frame}{Generalization Error}
\phantomsection\label{generalization-error}
\begin{itemize}
\tightlist
\item
  The \textbf{generalization error} (risk), \(R(h)\), measures how well
  the hypothesis \(h\) performs on unseen data. {[}12{]}
\item
  It's the expected value of the loss over the entire joint distribution
  \(P(X,Y)\). {[}12{]}
\end{itemize}
\end{frame}

\begin{frame}{Generalization Gap}
\phantomsection\label{generalization-gap}
\begin{itemize}
\tightlist
\item
  The \textbf{generalization gap} is the difference between the training
  error and the generalization error. {[}13{]}
\item
  It quantifies how well the performance on the training data
  generalizes to unseen data. {[}13{]}
\end{itemize}
\end{frame}

\section{Generalization Bounds}\label{generalization-bounds}

\begin{frame}{Motivation}
\phantomsection\label{motivation}
\begin{itemize}
\tightlist
\item
  \textbf{Generalization bounds} provide guarantees that the learned
  hypothesis will perform well on unseen data. {[}2, 14, 15{]}
\item
  They relate the generalization error to quantities we can observe or
  control, such as empirical risk, hypothesis space complexity, and
  dataset size. {[}16{]}
\end{itemize}
\end{frame}

\begin{frame}{Hoeffding's Inequality}
\phantomsection\label{hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  For a single hypothesis \(h\), Hoeffding's inequality bounds the
  difference between the empirical risk and the generalization error.
  {[}17, 18{]}
\end{itemize}
\end{frame}

\begin{frame}{Limitations of Hoeffding}
\phantomsection\label{limitations-of-hoeffding}
\begin{itemize}
\tightlist
\item
  Hoeffding's inequality doesn't directly apply to the entire hypothesis
  space. {[}8{]}
\item
  It only considers the boundedness of the functions, not their
  variance. {[}19{]}
\item
  The union bound, used to extend Hoeffding to multiple hypotheses,
  assumes all hypotheses are independent, which is not generally true.
  {[}20, 21{]}
\end{itemize}
\end{frame}

\section{Vapnik-Chervonenkis (VC)
Theory}\label{vapnik-chervonenkis-vc-theory}

\begin{frame}{The Union Bound}
\phantomsection\label{the-union-bound}
\begin{itemize}
\tightlist
\item
  The \textbf{union bound} extends the probability bounds to the entire
  hypothesis space. {[}22{]}
\item
  It states that the probability of at least one hypothesis having a
  large generalization gap is at most the sum of the probabilities of
  each individual hypothesis having a large gap. {[}22{]}
\end{itemize}
\end{frame}

\begin{frame}{The Growth Function}
\phantomsection\label{the-growth-function}
\begin{itemize}
\tightlist
\item
  The \textbf{growth function} quantifies the expressiveness of the
  hypothesis space. {[}23{]}
\item
  It's the maximum number of ways the hypothesis space can label a
  dataset of a given size. {[}23{]}
\end{itemize}
\end{frame}

\begin{frame}{VC Dimension}
\phantomsection\label{vc-dimension}
\begin{itemize}
\tightlist
\item
  The \textbf{VC dimension} is the largest dataset size that the
  hypothesis space can \textbf{shatter}. {[}24, 25{]}
\item
  Shattering means the hypothesis space can produce all possible
  labelings for the dataset. {[}24{]}
\item
  It's a measure of the complexity of the hypothesis space. {[}26{]}
\end{itemize}
\end{frame}

\begin{frame}{VC Generalization Bound}
\phantomsection\label{vc-generalization-bound}
\begin{itemize}
\tightlist
\item
  The \textbf{VC generalization bound} relates the generalization error
  to the empirical risk, VC dimension, and dataset size. {[}27{]}
\item
  It shows that the generalization error can be bounded by the empirical
  risk plus a term that depends on the VC dimension and dataset size.
  {[}28{]}
\end{itemize}
\end{frame}

\section{Beyond VC Dimension}\label{beyond-vc-dimension}

\begin{frame}{Distribution-Based Bounds}
\phantomsection\label{distribution-based-bounds}
\begin{itemize}
\tightlist
\item
  VC dimension is \textbf{distribution-free}, meaning it doesn't
  consider the data distribution. {[}29{]}
\item
  Tighter bounds can be obtained by considering the data distribution.
  {[}29, 30{]}
\item
  \textbf{Support Vector Machines (SVMs)} exemplify this by maximizing
  the margin between classes, which leads to a lower VC dimension and
  better generalization. {[}30{]}
\end{itemize}
\end{frame}

\begin{frame}{Other Capacity Measures}
\phantomsection\label{other-capacity-measures}
\begin{itemize}
\tightlist
\item
  \textbf{Covering numbers} measure the size of the hypothesis space
  using a metric based on the difference in predictions on the training
  data. {[}31{]}
\item
  \textbf{Rademacher complexity} measures how well the hypothesis space
  can fit random noise. {[}32, 33{]}
\item
  These measures can be used to derive generalization bounds. {[}32,
  34{]}
\end{itemize}
\end{frame}

\section{Tying it Together}\label{tying-it-together}

\begin{frame}{The General Form of Generalization Bounds}
\phantomsection\label{the-general-form-of-generalization-bounds}
\begin{itemize}
\tightlist
\item
  The general form of generalization bounds is:
  \(R(h) \leq R_{emp}(h) + C(|\mathcal{H}|, N, \delta)\). {[}35{]}
\item
  \(C(|\mathcal{H}|, N, \delta)\) represents a complexity term that
  depends on the hypothesis space complexity, dataset size, and the
  desired confidence level. {[}35{]}
\item
  It highlights the trade-off between minimizing the training error and
  controlling the model's complexity. {[}35{]}
\end{itemize}
\end{frame}

\begin{frame}{Relevance to Other Topics}
\phantomsection\label{relevance-to-other-topics}
\begin{block}{Rates of Convergence}
\phantomsection\label{rates-of-convergence}
\begin{itemize}
\tightlist
\item
  \textbf{Rates of convergence} quantify how fast the generalization
  error decreases with increasing sample size. {[}36{]}
\item
  They are closely linked to generalization bounds, as the bounds often
  provide insights into the rate of convergence. {[}37{]}
\end{itemize}
\end{block}

\begin{block}{PAC-Bayes}
\phantomsection\label{pac-bayes}
\begin{itemize}
\tightlist
\item
  \textbf{PAC-Bayes} offers a Bayesian approach to deriving
  generalization bounds. {[}38{]}
\end{itemize}
\end{block}
\end{frame}

\section{Conclusion}\label{conclusion}

\begin{frame}{Key Takeaways}
\phantomsection\label{key-takeaways}
\begin{itemize}
\tightlist
\item
  \textbf{Generalization bounds} are crucial for understanding and
  controlling the performance of machine learning models. {[}15{]}
\item
  They guarantee the learned hypothesis will perform well on unseen
  data. {[}15{]}
\end{itemize}
\end{frame}




\end{document}
