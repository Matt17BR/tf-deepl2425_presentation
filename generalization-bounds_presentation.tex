% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{part title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{part title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\usetheme[]{Dresden}
\usefonttheme{structurebold}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
\usepackage{caption}
% Make caption package work with longtable
\makeatletter
\def\fnum@table{\tablename~\thetable}
\makeatother
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\AtBeginSection{}
\titlegraphic{\includegraphics[width=0.33\textwidth]{lmu-logo.png}}
\definecolor{lmugreen}{RGB}{0,136,58}
\setbeamercolor{section in toc}{fg=black,bg=white}
\setbeamercolor{alerted text}{fg=lmugreen!80!gray}
\setbeamercolor*{palette primary}{fg=lmugreen!60!black,bg=gray!30!white}
\setbeamercolor*{palette secondary}{fg=lmugreen!70!black,bg=gray!15!white}
\setbeamercolor*{palette tertiary}{bg=lmugreen!80!black,fg=gray!10!white}
\setbeamercolor*{palette quaternary}{fg=lmugreen,bg=gray!5!white}
\setbeamercolor*{sidebar}{fg=lmugreen,bg=gray!15!white}
\setbeamercolor*{palette sidebar primary}{fg=lmugreen!10!black}
\setbeamercolor*{palette sidebar secondary}{fg=white}
\setbeamercolor*{palette sidebar tertiary}{fg=lmugreen!50!black}
\setbeamercolor*{palette sidebar quaternary}{fg=gray!10!white}
\setbeamercolor{titlelike}{parent=palette primary,fg=lmugreen}
\setbeamercolor{frametitle}{bg=lmugreen!10}
\setbeamercolor{frametitle right}{bg=gray!60!white}
\setbeamercolor{title}{bg=lmugreen!10, fg=lmugreen}
\setbeamercolor{item}{fg=lmugreen}
\setbeamercolor{enumerate item}{fg=lmugreen}
\setbeamercolor*{separation line}{}
\setbeamercolor*{fine separation line}{}
\setbeamertemplate{footline}%{miniframes theme}
{%
  \begin{beamercolorbox}[colsep=1.5pt]{upper separation line foot}
  \end{beamercolorbox}
  \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,%
    leftskip=.3cm,rightskip=.3cm plus1fil]{author in head/foot}%
    \leavevmode{\usebeamerfont{author in head/foot}\insertshortauthor}%
    \hfill%
    {\usebeamerfont{institute in head/foot}\usebeamercolor[fg]{institute in head/foot}\insertshortinstitute}%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,%
    leftskip=.3cm,rightskip=.3cm plus1fil]{title in head/foot}%
    {\usebeamerfont{title in head/foot}\insertshorttitle} \hfill     \insertframenumber/\inserttotalframenumber%
  \end{beamercolorbox}%
  \begin{beamercolorbox}[colsep=1.5pt]{lower separation line foot}
  \end{beamercolorbox}
}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Generalization Bounds},
  pdfauthor={Matteo Mazzarelli},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}


\title{Generalization Bounds}
\subtitle{Theoretical Foundations of Deep Learning}
\author{Matteo Mazzarelli}
\date{December 17, 2024}

\begin{document}
\frame{\titlepage}


\section{Introduction}\label{introduction}

\begin{frame}{Motivation}
\phantomsection\label{motivation}
\begin{itemize}
\tightlist
\item
  \textbf{Core Challenge}: How can a model learned from \emph{limited
  training data} perform well on \emph{unseen data}?
\item
  Generalization lies at the heart of the machine learning process.
\item
  A poorly generalized model risks:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Overfitting}: Performing well on training data but poorly on
    unseen data.
  \item
    \textbf{Underfitting}: Failing to capture the underlying patterns of
    the data.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{The Learning Problem}
\phantomsection\label{the-learning-problem}
\begin{itemize}
\tightlist
\item
  \textbf{Supervised Learning}:

  \begin{itemize}
  \tightlist
  \item
    Goal: Learn a function \(f: X \to Y\) mapping inputs \(X\) to
    outputs \(Y\) based on labeled training data.
  \end{itemize}
\item
  \textbf{Key Question}: Can the learned function perform well on unseen
  data?
\item
  \textbf{Generalization}:

  \begin{itemize}
  \tightlist
  \item
    Ability of a model to extend its learning beyond the training data.
  \item
    \textbf{Central Problem} in machine learning: balancing
    \emph{empirical performance} with \emph{future predictions}.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Why Theory Matters}
\phantomsection\label{why-theory-matters}
\begin{itemize}
\tightlist
\item
  \textbf{Significance of Theory}:

  \begin{itemize}
  \tightlist
  \item
    Guides \textbf{algorithm design} by providing a foundation for
    developing new methods.
  \item
    Allows \textbf{performance analysis} to identify the strengths and
    weaknesses of algorithms.
  \item
    Reveals \textbf{limitations} of learning systems, helping us
    understand their boundaries.
  \end{itemize}
\item
  \textbf{Theoretical Understanding}:

  \begin{itemize}
  \tightlist
  \item
    Bridges the gap between empirical performance and guarantees on
    future behavior.
  \end{itemize}
\end{itemize}
\end{frame}

\section{Overfitting}\label{overfitting}

\begin{frame}[fragile]{Simulating Overfitting}
\phantomsection\label{simulating-overfitting}
\begin{itemize}
\tightlist
\item
  \textbf{Objective}:

  \begin{itemize}
  \tightlist
  \item
    Visualize the impact of model complexity on overfitting in a linear
    regression model.
  \end{itemize}
\item
  \textbf{Dataset}

  \begin{itemize}
  \tightlist
  \item
    The experiment uses the \textbf{Boston Housing dataset}, where the
    target variable is \texttt{medv} (median value of owner-occupied
    homes), and the features represent housing characteristics.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Experimental Setup}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Model Complexity}:

    \begin{itemize}
    \tightlist
    \item
      Complexity is defined by the number of features included in the
      model.
    \item
      Additional random features are generated to simulate increasing
      complexity beyond the real features in the dataset.
    \end{itemize}
  \item
    \textbf{Train-Test Split}:

    \begin{itemize}
    \tightlist
    \item
      The dataset is split into 70\% training and 30\% test data.
    \end{itemize}
  \item
    \textbf{Range of Complexity}:

    \begin{itemize}
    \tightlist
    \item
      Models are trained with 1 to 200 features. Beyond the actual
      features in the dataset, random noise features are added
      incrementally.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Procedure}: For each level of complexity:

  \begin{enumerate}
  \tightlist
  \item
    A subset of features (real and random) is used to train a linear
    regression model.
  \item
    Predictions are made on both the training and test datasets.
  \item
    Mean Squared Errors (MSE) are calculated for both datasets.
  \end{enumerate}
\item
  \textbf{Results}:

  \begin{itemize}
  \tightlist
  \item
    Training error decreases consistently as model complexity increases.
  \item
    Test error initially decreases but then increases, demonstrating the
    overfitting phenomenon.
  \end{itemize}
\item
  \textbf{Visualization}:

  \begin{itemize}
  \tightlist
  \item
    A line plot shows the relationship between model complexity (number
    of features) and mean squared error for both the training and test
    datasets.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{figure}[H]

{\centering \includegraphics[width=0.9\textwidth,height=0.9\textheight]{generalization-bounds_presentation_files/figure-beamer/overfitting-plot-1.pdf}

}

\caption{Overfitting Phenomenon in Linear Regression}

\end{figure}%

\begin{itemize}
\tightlist
\item
  \textbf{Highlights}:

  \begin{itemize}
  \tightlist
  \item
    The \textbf{bias-variance tradeoff}.
  \item
    The point where overfitting begins, indicated by the divergence of
    training and test errors.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights}
\begin{itemize}
\tightlist
\item
  Increasing model complexity without consideration of the underlying
  data structure can lead to overfitting.
\item
  Simple models that focus on the true underlying pattern often
  generalize better to unseen data.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Double Descent}
\phantomsection\label{double-descent}
\begin{itemize}
\tightlist
\item
  However, modern machine learning introduces a fascinating twist:
  \textbf{Double Descent}, where increasing model complexity can
  sometimes lead to improved generalization after an initial overfitting
  phase.
\item
  Unlike traditional models where increasing complexity leads to
  overfitting, further increasing the complexity (e.g., using
  overparameterized neural networks) can eventually reduce the
  generalization error after an initial peak.
\item
  This challenges the classical view of overfitting and highlights the
  complex relationship between model complexity and generalization in
  modern machine learning.
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights-1}
\begin{itemize}
\tightlist
\item
  While simple models often underfit and overly complex models overfit,
  the phenomenon of \textbf{Double Descent} shows that extremely complex
  models can sometimes achieve superior generalization, especially in
  overparameterized regimes.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Introducing Generalization Bounds}
\phantomsection\label{introducing-generalization-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{What Are Generalization Bounds?}

  \begin{itemize}
  \tightlist
  \item
    Theoretical tools offering guarantees about a model's performance on
    unseen data.
  \item
    Relate:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Generalization Error}: How well the model performs on
      unseen data.
    \item
      \textbf{Empirical Risk}: Performance observed on training data.
    \item
      \textbf{Model Complexity}: How expressive the model is.
    \item
      \textbf{Approximation Limits}: What kinds of functions the model
      class can represent, as explained by Approximation Theory.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Purpose}:

  \begin{itemize}
  \tightlist
  \item
    Provide insights into the trade-offs between:

    \begin{itemize}
    \tightlist
    \item
      \textbf{Model Accuracy}: How well the model captures the data
      patterns.
    \item
      \textbf{Model Complexity}: The expressiveness of the model and its
      ability to fit intricate patterns.
    \item
      \textbf{Training Data Size}: How much data is required to achieve
      reliable generalization.
    \end{itemize}
  \item
    Approximation Theory informs this by helping us understand the
    limits of a model class:

    \begin{itemize}
    \tightlist
    \item
      Simpler models may not be able to represent complex data (high
      approximation error).
    \item
      Overly complex models risk overfitting, where generalization error
      increases despite better fit on training data.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\section{Exploring Bounds}\label{exploring-bounds}

\begin{frame}{Hoeffding's Inequality: A Starting Point}
\phantomsection\label{hoeffdings-inequality-a-starting-point}
\begin{itemize}
\tightlist
\item
  \textbf{What is Hoeffding's Inequality?}

  \begin{itemize}
  \tightlist
  \item
    A fundamental result in probability theory used to bound the
    difference between the \textbf{empirical risk} and the
    \textbf{generalization error} for a fixed hypothesis.
  \item
    Provides a way to measure how closely a model's performance on
    training data reflects its performance on unseen data.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Mathematical Formulation of Hoeffding's Inequality}
\phantomsection\label{mathematical-formulation-of-hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  \textbf{Hoeffding's Inequality}: \[
  P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
  \]

  \begin{itemize}
  \tightlist
  \item
    \(R(h)\): Generalization error (true performance on unseen data).
  \item
    \(R_{\text{emp}}(h)\): Empirical risk (error on training data).
  \item
    \(\varepsilon\): A small positive value (tolerance).
  \item
    \(m\): Size of the dataset.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{block}{Key Insights}
\phantomsection\label{key-insights-2}
\begin{itemize}
\tightlist
\item
  The probability that the generalization error \(R(h)\) deviates
  significantly from the empirical risk \(R_{\text{emp}}(h)\) decreases
  \textbf{exponentially} with:

  \begin{itemize}
  \tightlist
  \item
    Larger dataset size \(m\).
  \item
    Smaller tolerance \(\varepsilon\).
  \end{itemize}
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Rates of Convergence}
\phantomsection\label{rates-of-convergence}
\begin{itemize}
\tightlist
\item
  \textbf{What Are Rates of Convergence?}

  \begin{itemize}
  \tightlist
  \item
    Quantify how quickly the generalization error approaches the
    empirical risk as the dataset size \(m\) grows.
  \item
    In Hoeffding's inequality: \[
    P(|R(h) - R_{\text{emp}}(h)| > \varepsilon) \leq 2 \exp(-2m\varepsilon^2)
    \]

    \begin{itemize}
    \tightlist
    \item
      The \textbf{exponential term} \(\exp(-2m\varepsilon^2)\) shows
      that the convergence is faster with larger datasets.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Key Factors}:

  \begin{itemize}
  \tightlist
  \item
    \textbf{Dataset Size (\(m\))}: Larger datasets reduce the gap
    between \(R(h)\) and \(R_{\text{emp}}(h)\) more quickly.
  \item
    \textbf{Tolerance (\(\varepsilon\))}: Smaller tolerances require
    larger datasets for the same level of confidence.
  \end{itemize}
\item
  \textbf{Practical Insight}:

  \begin{itemize}
  \tightlist
  \item
    Rates of convergence provide a guideline for determining the dataset
    size needed to achieve a desired level of generalization.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Interpretation of Hoeffding's Inequality}
\phantomsection\label{interpretation-of-hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  \textbf{What Does It Mean?}

  \begin{itemize}
  \tightlist
  \item
    As the dataset size (\(m\)) increases, the empirical risk becomes a
    more reliable indicator of the generalization error.
  \item
    For a fixed hypothesis, we can be confident that the performance
    observed on training data is close to what can be expected on unseen
    data.
  \item
    The \textbf{rate of convergence} shows how quickly this reliability
    improves as \(m\) grows.
  \end{itemize}
\end{itemize}

\begin{block}{Key Insights}
\phantomsection\label{key-insights-3}
\begin{itemize}
\tightlist
\item
  Hoeffding's inequality gives a \textbf{quantitative guarantee} about
  the relationship between training performance and unseen data
  performance.
\item
  Understanding convergence rates helps in planning how much data is
  needed for robust generalization.
\end{itemize}
\end{block}
\end{frame}

\begin{frame}{Limitations of Hoeffding's Inequality}
\phantomsection\label{limitations-of-hoeffdings-inequality}
\begin{itemize}
\tightlist
\item
  \textbf{The Challenge of Multiple Hypotheses}:

  \begin{itemize}
  \tightlist
  \item
    In practical machine learning, we often choose the best hypothesis
    from a large hypothesis class \(\mathcal{H}\).
  \item
    Hoeffding's inequality applies to a \textbf{single fixed
    hypothesis}, not to the case where multiple hypotheses are
    considered.
  \end{itemize}
\item
  \textbf{Implication}:

  \begin{itemize}
  \tightlist
  \item
    It doesn't directly address:

    \begin{itemize}
    \tightlist
    \item
      The \textbf{selection bias} introduced by choosing the hypothesis
      that minimizes the empirical risk.
    \item
      The increased risk of overfitting when evaluating multiple
      hypotheses.
    \end{itemize}
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Limitations of Hoeffding's Inequality}
\phantomsection\label{limitations-of-hoeffdings-inequality-1}
\begin{itemize}
\tightlist
\item
  \textbf{Beyond Fixed Hypotheses}:

  \begin{itemize}
  \tightlist
  \item
    Hoeffding's inequality assumes a single, fixed hypothesis.
  \item
    In practice, machine learning involves selecting the best hypothesis
    from a \textbf{large hypothesis class} \(\mathcal{H}\), increasing
    the risk of overfitting.
  \end{itemize}
\item
  \textbf{Need for Complexity-Aware Bounds}:

  \begin{itemize}
  \tightlist
  \item
    Simple bounds like Hoeffding's do not consider the complexity of the
    hypothesis class, which influences generalization.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Motivation for Advanced Bounds}
\phantomsection\label{motivation-for-advanced-bounds}
\begin{itemize}
\tightlist
\item
  \textbf{Advanced bounds} address this by incorporating:

  \begin{itemize}
  \tightlist
  \item
    \textbf{VC Dimension}: A measure of the capacity or expressiveness
    of a hypothesis class. Higher VC dimensions indicate more complex
    models, which may require more data to generalize well.
  \item
    \textbf{Rademacher Complexity}: A data-dependent measure of how well
    a hypothesis class can fit random noise in the training data. It
    captures both the hypothesis class and the specifics of the data
    distribution.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\tightlist
\item
  \textbf{Extending Convergence Rates}:

  \begin{itemize}
  \tightlist
  \item
    Advanced bounds refine the rates of convergence by linking the
    generalization error to:

    \begin{itemize}
    \tightlist
    \item
      The size of the dataset \(m\).
    \item
      The complexity of the hypothesis class (e.g., \textbf{VC
      dimension} or \textbf{Rademacher complexity}).
    \end{itemize}
  \item
    For example, the generalization error is often bounded as: \[
    R(h) - R_{\text{emp}}(h) \leq \mathcal{O}\left(\sqrt{\frac{\text{Complexity}(\mathcal{H})}{m}}\right)
    \]

    \begin{itemize}
    \tightlist
    \item
      Larger datasets \(m\) reduce error, but higher complexity
      increases the required data for a desired level of generalization.
    \end{itemize}
  \end{itemize}
\item
  \textbf{Practical Implications}:

  \begin{itemize}
  \tightlist
  \item
    These bounds provide actionable insights for balancing model
    complexity and dataset size.
  \end{itemize}
\end{itemize}
\end{frame}




\end{document}
